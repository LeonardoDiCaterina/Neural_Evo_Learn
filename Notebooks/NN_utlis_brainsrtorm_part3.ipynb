{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Networks - Learning and Implementation strategies\n",
    "\n",
    "In this notebook, we will study the use of `PyTorch` and `Tensorflow` frameworks for implementing and training Neural Networks. This is not intended to be exhaustive, but rather to provide examples for exploring the algorithms and their hyperparameters with these frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Net_group_Y(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=2, output_size=1,\n",
    "                 hidden_layer_sizes=[3,4,5], activation=nn.ReLU()):\n",
    "        \n",
    "        super(Net_group_Y, self).__init__()\n",
    "        #\n",
    "        # 1. 1. Network architecture\n",
    "        \n",
    "        self.add_module(f'fc{1}', nn.Linear(input_size, hidden_layer_sizes[0]))\n",
    "        \n",
    "        for i in range(1,len(hidden_layer_sizes)):\n",
    "            self.add_module(f'fc{i+1}', nn.Linear(hidden_layer_sizes[i-1], hidden_layer_sizes[i]))\n",
    "        \n",
    "        self.add_module(f'fc{len(hidden_layer_sizes) +1 }', nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "\n",
    "        # Weights initialisation\n",
    "        # The apply method applies the function passed as the apply() argument\n",
    "        # to each element in the object, that in this case is the neural network.\n",
    "        self.apply(self._init_weights)\n",
    "        # Store the parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.n_forward_calls = 0\n",
    "        \n",
    "    #\n",
    "    # 1. 2. Weights and bias initialization\n",
    "    #\n",
    "    def _init_weights(self, attribute):\n",
    "        if isinstance(attribute, nn.Linear):\n",
    "          torch.nn.init.xavier_uniform_(attribute.weight)\n",
    "          torch.nn.init.zeros_(attribute.bias)\n",
    "    #\n",
    "    # 1. 3. Forward pass\n",
    "    \"\"\"    \n",
    "    def forward(self, x):\n",
    "        # For each layer, the output will be the ReLu activation applied to the output of the linear operation\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # For the last layer, the sigmoid function will be the activation\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through all layers\n",
    "        for i in range(1, len(self.hidden_layer_sizes) + 2):\n",
    "            #print(f'forward pass layer {i}')\n",
    "            layer = getattr(self, f'fc{i}')\n",
    "            x = layer(x)\n",
    "            if i < len(self.hidden_layer_sizes):\n",
    "                self.n_forward_calls += 1\n",
    "                x = self.activation(x)\n",
    "        # Apply sigmoid activation to the output layer\n",
    "        self.n_forward_calls += 1\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    #\n",
    "    # 1. 4. Training loop\n",
    "    # For details, see Machine Learning with PyTorch and Scikit-Learn.\n",
    "    #\n",
    "    def train(self, num_epochs, loss_fn, optimizer, train_dl, train_size, batch_size, x_valid, y_valid):\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "        # Loss and accuracy history objects initialization\n",
    "        loss_hist_train = [0] * num_epochs\n",
    "        accuracy_hist_train = [0] * num_epochs\n",
    "        loss_hist_valid = [0] * num_epochs\n",
    "        accuracy_hist_valid = [0] * num_epochs\n",
    "        delta_times = [0] * num_epochs\n",
    "        self.n_forward_calls = 0\n",
    "        \n",
    "        # Learning loop\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            start_time = time.time()\n",
    "            # Batch learn\n",
    "            for x_batch, y_batch in train_dl:\n",
    "                #print('*'*20)\n",
    "                # ---\n",
    "                # 1.4.1. Get the predictions, the [:,0] reshapes from (batch_size,1) to (batch_size)\n",
    "                pred = self(x_batch)[:,0]\n",
    "                # 1.4.2. Compute the loss\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                # 1.4.3. Back propagate the gradients\n",
    "                # The `backward()` method, already available in PyTroch, calculates the \n",
    "                # derivative of the Error in respect to the NN weights\n",
    "                # applying the chain rule for hidden neurons.\n",
    "                loss.backward()\n",
    "                # 1.4.4. Update the weights based on the computed gradients\n",
    "                optimizer.step()\n",
    "                # ---\n",
    "                \n",
    "                # Reset to zero the gradients so they will not accumulate over the mini-batches\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update performance metrics\n",
    "                loss_hist_train[epoch] += loss.item()\n",
    "                is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "                accuracy_hist_train[epoch] += is_correct.mean()\n",
    "            \n",
    "            delta_times[epoch] = time.time() - start_time\n",
    "            # Average the results\n",
    "            loss_hist_train[epoch] /= train_size/batch_size\n",
    "            accuracy_hist_train[epoch] /= train_size/batch_size\n",
    "            \n",
    "            # Predict the validation set\n",
    "            pred = self(x_valid)[:, 0]\n",
    "            loss_hist_valid[epoch] = loss_fn(pred, y_valid).item()\n",
    "            is_correct = ((pred>=0.5).float() == y_valid).float()\n",
    "            accuracy_hist_valid[epoch] += is_correct.mean()\n",
    "            \n",
    "        return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid, self.n_forward_calls, delta_times\n",
    "\n",
    "    # Not needed normaly, it is just for mlextend plots\n",
    "    def predict(self, x):\n",
    "        print(f'predict with input shape: {x.shape}')\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        pred = self.forward(x)[:, 0]\n",
    "        print(f'finished predict with output shape: {pred.shape}')\n",
    "        return (pred>=0.5).float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "\n",
    "criterion = RMSELoss()\n",
    "\n",
    "class nn_model():\n",
    "    def __init__(self,\n",
    "                X_train,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_val,\n",
    "                input_size, \n",
    "                output_size,\n",
    "                hidden_layer_sizes, \n",
    "                optimizer,\n",
    "                num_epochs,\n",
    "                train_dl,  \n",
    "                train_size,\n",
    "                batch_size,\n",
    "                learning_rate,\n",
    "                activation=nn.ReLU(),\n",
    "                loss_fn = RMSELoss()\n",
    "                ):\n",
    "        \n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.arcthicture = Net_group_Y(input_size=input_size,\n",
    "                                  output_size=output_size,\n",
    "                                  hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                  activation=activation)\n",
    "        \n",
    "        self.arcthicture.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.fitness, self.test_fitness, self.accuracy, self.accuracy_valid, self.n_forward_calls, self.delta_times = self.fit(\n",
    "                X_train,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_val,\n",
    "                num_epochs=num_epochs, \n",
    "                loss_fn=criterion, \n",
    "                optimizer_name=optimizer,\n",
    "                batch_size=batch_size, \n",
    "                learning_rate=learning_rate)\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "    def fit(self,X_train,X_val,y_train,y_val,\n",
    "                num_epochs, loss_fn, optimizer_name, batch_size, learning_rate):\n",
    "        \"\"\"\n",
    "        Train the model with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_epochs: Number of epochs to train the model.\n",
    "        - loss_fn: Loss function to use for training.\n",
    "        - optimizer: Optimizer to use for training.\n",
    "        - batch_size: Size of each batch during training.\n",
    "        - x_valid: Validation input data.\n",
    "        - y_valid: Validation target data.\n",
    "        - learning_rate: Learning rate for the optimizer.\n",
    "        \n",
    "        Returns:\n",
    "        - history: Training history containing loss and accuracy metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define datasets for data loaders\n",
    "        train_ds = TensorDataset(X_train, y_train)\n",
    "        val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "        train_size = len(train_ds)\n",
    "        if optimizer_name == 'GD':\n",
    "            batch_size = train_size\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "            #val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "        \n",
    "        elif optimizer_name == 'SGD':\n",
    "            batch_size = 1\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "            #val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "\n",
    "        else:\n",
    "            batch_size = batch_size\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "            #val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "        \n",
    "        optimizer_choiche = {\n",
    "        'GD': torch.optim.SGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'SGD': torch.optim.SGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'MiniSGD': torch.optim.SGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'ASGD': torch.optim.ASGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        \n",
    "        'RMSprop': torch.optim.RMSprop(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'Adam': torch.optim.Adam(self.arcthicture.parameters(), lr=learning_rate)\n",
    "        }\n",
    "        optimizer_instance = optimizer_choiche[optimizer_name]\n",
    "        \n",
    "        return self.arcthicture.train(\n",
    "            num_epochs=num_epochs, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optimizer_instance, \n",
    "            train_dl=train_dl, \n",
    "            train_size=train_size, \n",
    "            batch_size=batch_size,\n",
    "            x_valid=val_ds[0],\n",
    "            y_valid=val_ds[1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes\n",
      "[torch.Size([150, 2]), torch.Size([150])] [torch.float32, torch.float32] [device(type='cpu'), device(type='cpu')]\n",
      "[torch.Size([50, 2]), torch.Size([50])] [torch.float32, torch.float32] [device(type='cpu'), device(type='cpu')]\n",
      "\n",
      "Batch sizes\n",
      "150\n",
      "1\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Data simulation\n",
    "N_data = 200\n",
    "train_size = 150\n",
    "X = 2 * torch.rand(N_data, 2, device=device, dtype=torch.float32) - 1\n",
    "y = torch.tensor([0 if elem[0]*elem[1] < 0 else 1 for elem in X], device=device, dtype=torch.float32)\n",
    "\n",
    "# Split training and test partitions\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:, :]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "# Define datasets for data loaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size_GD = train_size\n",
    "train_dl_GD = DataLoader(train_ds, batch_size_GD, shuffle=True)\n",
    "val_dl_GD = DataLoader(val_ds, batch_size_GD, shuffle=True)\n",
    "\n",
    "batch_size_SGD = 1\n",
    "train_dl_SGD = DataLoader(train_ds, batch_size_SGD, shuffle=True)\n",
    "val_dl_SGD = DataLoader(val_ds, batch_size_SGD, shuffle=True)\n",
    "\n",
    "batch_size_MiniSGD = 32\n",
    "train_dl_MniSGD = DataLoader(train_ds, batch_size_MiniSGD, shuffle=True)\n",
    "val_dl_MiniSGD = DataLoader(val_ds, batch_size_MiniSGD, shuffle=True)\n",
    "\n",
    "print('Partition sizes')\n",
    "print([X_train.shape, y_train.shape], [X_train.dtype, y_train.dtype], [X_train.device, y_train.device])\n",
    "print([X_val.shape, y_val.shape], [X_val.dtype, y_val.dtype], [X_val.device, y_val.device])\n",
    "\n",
    "print('\\nBatch sizes')\n",
    "print(batch_size_GD)\n",
    "print(batch_size_SGD)\n",
    "print(batch_size_MiniSGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (150x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nn_model(X_train \u001b[38;5;241m=\u001b[39m X_train,\n\u001b[0;32m      2\u001b[0m          X_val \u001b[38;5;241m=\u001b[39m X_val,\n\u001b[0;32m      3\u001b[0m          y_train \u001b[38;5;241m=\u001b[39m y_train,\n\u001b[0;32m      4\u001b[0m         y_val \u001b[38;5;241m=\u001b[39m y_val,\n\u001b[0;32m      5\u001b[0m         input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, \n\u001b[0;32m      6\u001b[0m         output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      7\u001b[0m         hidden_layer_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m], \n\u001b[0;32m      8\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGD\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m         num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     10\u001b[0m         train_dl \u001b[38;5;241m=\u001b[39m train_dl_GD,  \n\u001b[0;32m     11\u001b[0m         train_size \u001b[38;5;241m=\u001b[39m train_size,\n\u001b[0;32m     12\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m batch_size_GD,\n\u001b[0;32m     13\u001b[0m         learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m     14\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[27], line 46\u001b[0m, in \u001b[0;36mnn_model.__init__\u001b[1;34m(self, X_train, X_val, y_train, y_val, input_size, output_size, hidden_layer_sizes, optimizer, num_epochs, train_dl, train_size, batch_size, learning_rate, activation, loss_fn)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;241m=\u001b[39m num_epochs\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_fitness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy_valid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forward_calls, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     47\u001b[0m         X_train,\n\u001b[0;32m     48\u001b[0m         X_val,\n\u001b[0;32m     49\u001b[0m         y_train,\n\u001b[0;32m     50\u001b[0m         y_val,\n\u001b[0;32m     51\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, \n\u001b[0;32m     52\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mcriterion, \n\u001b[0;32m     53\u001b[0m         optimizer_name\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     54\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m     55\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "Cell \u001b[1;32mIn[27], line 109\u001b[0m, in \u001b[0;36mnn_model.fit\u001b[1;34m(self, X_train, X_val, y_train, y_val, num_epochs, loss_fn, optimizer_name, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     98\u001b[0m optimizer_choiche \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGD\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marcthicture\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate),\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marcthicture\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marcthicture\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m    106\u001b[0m }\n\u001b[0;32m    107\u001b[0m optimizer_instance \u001b[38;5;241m=\u001b[39m optimizer_choiche[optimizer_name]\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marcthicture\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    110\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, \n\u001b[0;32m    111\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, \n\u001b[0;32m    112\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer_instance, \n\u001b[0;32m    113\u001b[0m     train_dl\u001b[38;5;241m=\u001b[39mtrain_dl, \n\u001b[0;32m    114\u001b[0m     train_size\u001b[38;5;241m=\u001b[39mtrain_size, \n\u001b[0;32m    115\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    116\u001b[0m     x_valid\u001b[38;5;241m=\u001b[39mval_ds[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    117\u001b[0m     y_valid\u001b[38;5;241m=\u001b[39mval_ds[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[26], line 86\u001b[0m, in \u001b[0;36mNet_group_Y.train\u001b[1;34m(self, num_epochs, loss_fn, optimizer, train_dl, train_size, batch_size, x_valid, y_valid)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Batch learn\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m#print('*'*20)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# ---\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# 1.4.1. Get the predictions, the [:,0] reshapes from (batch_size,1) to (batch_size)\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x_batch)[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# 1.4.2. Compute the loss\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch)\n",
      "File \u001b[1;32mc:\\Users\\irism\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\irism\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[26], line 53\u001b[0m, in \u001b[0;36mNet_group_Y.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_sizes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#print(f'forward pass layer {i}')\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_sizes):\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_forward_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\irism\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\irism\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\irism\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (150x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "nn_model(X_train = X_train,\n",
    "         X_val = X_val,\n",
    "         y_train = y_train,\n",
    "        y_val = y_val,\n",
    "        input_size = 3, \n",
    "        output_size = 1,\n",
    "        hidden_layer_sizes = [2,3,4], \n",
    "        optimizer = 'GD',\n",
    "        num_epochs = 10,\n",
    "        train_dl = train_dl_GD,  \n",
    "        train_size = train_size,\n",
    "        batch_size = batch_size_GD,\n",
    "        learning_rate = 0.01\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the time of the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irism\\OneDrive - NOVAIMS\\Msc-NEL\\Neural_Evo_Learn\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(os.path.join(os.getcwd(), os.pardir))\n",
    "%cd notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_times = {}\n",
    "for model in ['SLIM-GSGP', 'GSGP', 'GP']:\n",
    "    total_time = 0\n",
    "    for i_outer in range(10):\n",
    "        df = pd.read_csv(f\"./log/{model}/{model}_sustavianfeed_outer_{i_outer}.csv\")\n",
    "        total_time += df.iloc[:, 6].sum()\n",
    "    model_times[model] = total_time\n",
    "\n",
    "model_times_df = pd.DataFrame.from_dict(model_times, orient='index', columns=['Total Time (s)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM-GSGP</th>\n",
       "      <td>24.840032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSGP</th>\n",
       "      <td>44.637842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GP</th>\n",
       "      <td>144.934460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Total Time (s)\n",
       "SLIM-GSGP       24.840032\n",
       "GSGP            44.637842\n",
       "GP             144.934460"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_times_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
