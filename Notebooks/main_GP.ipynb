{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/LeonardoDiCaterina/Neural_Evo_Learn"
      ],
      "metadata": {
        "id": "uaER3-np4nSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b572ce8a-4aa7-4289-fe37-c18de86742fb"
      },
      "id": "uaER3-np4nSW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Neural_Evo_Learn'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (118/118), done.\u001b[K\n",
            "remote: Total 152 (delta 53), reused 126 (delta 27), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (152/152), 4.17 MiB | 7.35 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/DALabNOVA/slim.git"
      ],
      "metadata": {
        "id": "vqtnfqd47-Nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2423d600-2fff-4367-c1bb-e5b92cc50362"
      },
      "id": "vqtnfqd47-Nm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/DALabNOVA/slim.git\n",
            "  Cloning https://github.com/DALabNOVA/slim.git to /tmp/pip-req-build-_ybws08a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/DALabNOVA/slim.git /tmp/pip-req-build-_ybws08a\n",
            "  Resolved https://github.com/DALabNOVA/slim.git to commit de744d202ce166cc6b7d983bcef84073ea6bdcc7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from slim_gsgp==0.1.10) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from slim_gsgp==0.1.10) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from slim_gsgp==0.1.10) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from slim_gsgp==0.1.10) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from slim_gsgp==0.1.10) (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from slim_gsgp==0.1.10) (1.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->slim_gsgp==0.1.10) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->slim_gsgp==0.1.10) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->slim_gsgp==0.1.10) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->slim_gsgp==0.1.10) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->slim_gsgp==0.1.10)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->slim_gsgp==0.1.10) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->slim_gsgp==0.1.10) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->slim_gsgp==0.1.10) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->slim_gsgp==0.1.10) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: slim_gsgp\n",
            "  Building wheel for slim_gsgp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for slim_gsgp: filename=slim_gsgp-0.1.10-py3-none-any.whl size=1137878 sha256=03d11f3388c81ab5eb4a18028f4560eeead3b7533d280f2ad9baee8b2e4387bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z0n7jn24/wheels/7e/85/a7/72b9148abb510f97bab4c3231810b4a8015cea421ad858a30a\n",
            "Successfully built slim_gsgp\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, slim_gsgp\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 slim_gsgp-0.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83deb505",
      "metadata": {
        "id": "83deb505"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import itertools\n",
        "import json\n",
        "import datetime\n",
        "import pathlib\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Slim-GSGP imports\n",
        "from slim_gsgp.datasets.data_loader import load_pandas_df\n",
        "from slim_gsgp.utils.utils import train_test_split\n",
        "from slim_gsgp.main_gp import gp\n",
        "#from slim_gsgp.main_gsgp import gsgp\n",
        "#from slim_gsgp.main_slim import slim\n",
        "from slim_gsgp.evaluators.fitness_functions import rmse\n",
        "\n",
        "import statistics\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0410435f",
      "metadata": {
        "id": "0410435f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# os.chdir(os.path.join(os.getcwd(), os.pardir))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207cfae3",
      "metadata": {
        "id": "207cfae3"
      },
      "source": [
        "## Aux Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb91e0c",
      "metadata": {
        "id": "ccb91e0c"
      },
      "outputs": [],
      "source": [
        "def fit_model_GridSearch(gp_model, fixed_params, param_grid, seed):\n",
        "    models = []\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    for combo in product(*values):\n",
        "        dynamic_params = dict(zip(keys, combo))\n",
        "        full_params = {**fixed_params, **dynamic_params}\n",
        "        model = gp_model(**full_params, seed=seed)\n",
        "        res = {'model': model}\n",
        "        res.update({'rmse_train': model.fitness.item()})\n",
        "        res.update({'rmse_test': model.test_fitness.item()})\n",
        "        res.update({'dynamic_params': dynamic_params})\n",
        "        models.append(res)\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a86130f",
      "metadata": {
        "id": "7a86130f"
      },
      "outputs": [],
      "source": [
        "def group_and_median_rmse(results_data):\n",
        "    \"\"\"\n",
        "    Groups results by 'dynamic_params' and calculates the median 'rmse_test' for each group.\n",
        "\n",
        "    Args:\n",
        "        results_data (list): A list of lists, where each inner list contains dictionaries\n",
        "                             with 'dynamic_params' and 'rmse_test' keys.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing:\n",
        "              {'dynamic_params': {...}, 'rmse_test_median': float}\n",
        "    \"\"\"\n",
        "\n",
        "    #Flatten the list of lists into a single list of dictionaries\n",
        "    flattened_results = list(itertools.chain.from_iterable(results_data))\n",
        "\n",
        "\n",
        "    grouped_scores_data = {}\n",
        "\n",
        "    for item in flattened_results:\n",
        "        dynamic_params_dict = item['dynamic_params']\n",
        "        rmse_test = item['rmse_test']\n",
        "\n",
        "        # Sort params to ensure consistency\n",
        "        # Convert to tuple to make it hashable, and so able to be used as a dictionary key\n",
        "        hashable_dynamic_params = tuple(sorted(dynamic_params_dict.items()))\n",
        "\n",
        "        # Check if combination does not exist in the dictionary\n",
        "        if hashable_dynamic_params not in grouped_scores_data:\n",
        "\n",
        "            # Create entry if not\n",
        "            grouped_scores_data[hashable_dynamic_params] = {\n",
        "                'dynamic_params': dynamic_params_dict,\n",
        "                'rmse_test': []\n",
        "            }\n",
        "        grouped_scores_data[hashable_dynamic_params]['rmse_test'].append(rmse_test)\n",
        "\n",
        "    # Calculate median for each group and format output\n",
        "    final_output = []\n",
        "    for group_info in grouped_scores_data.values():\n",
        "        combination = group_info['dynamic_params']\n",
        "        rmse_scores = group_info['rmse_test']\n",
        "\n",
        "        # Calculate median RMSE\n",
        "        median_rmse = statistics.median(rmse_scores)\n",
        "        final_output.append({\n",
        "            'dynamic_params': combination,\n",
        "            'rmse_test_median': median_rmse\n",
        "        })\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80efa210",
      "metadata": {
        "id": "80efa210"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Neural_Evo_Learn/"
      ],
      "metadata": {
        "id": "TUYC9aTm9Abm",
        "outputId": "48ff4537-1222-4418-c830-088f0803014b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TUYC9aTm9Abm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Neural_Evo_Learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820ba804",
      "metadata": {
        "id": "820ba804"
      },
      "outputs": [],
      "source": [
        "# Reading the desired dataset\n",
        "df = pd.read_csv(\"Data/sustavianfeed.csv\", sep=';')\n",
        "\n",
        "# Dropping the first column (index) and renaming the columns\n",
        "df = df.drop(columns= ['WING TAG', 'EMPTY MUSCULAR STOMACH'])\n",
        "\n",
        "# Moving crude protein to the end of the dataframe\n",
        "df = df[[col for col in df.columns if col != 'CRUDE PROTEIN'] + ['CRUDE PROTEIN']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f3f831",
      "metadata": {
        "id": "28f3f831"
      },
      "source": [
        "# Nested CV with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c6c744",
      "metadata": {
        "id": "47c6c744"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "# Edit the name and log directory based on the model you want to run\n",
        "\n",
        "MODEL_NAME = 'GP'\n",
        "#MODEL_NAME = 'GSGP'\n",
        "#MODEL_NAME = 'SLIM-GSGP'\n",
        "\n",
        "DATASET_NAME = MODEL_NAME +'_sustavianfeed'\n",
        "LOG_DIR = './log/' + MODEL_NAME + '/'\n",
        "\n",
        "LOG_LEVEL = 2\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5007d76",
      "metadata": {
        "id": "b5007d76"
      },
      "outputs": [],
      "source": [
        "k_outer = 10\n",
        "k_inner = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d600cb87",
      "metadata": {
        "id": "d600cb87"
      },
      "outputs": [],
      "source": [
        "# Turning df into X and y torch.Tensors\n",
        "X, y = load_pandas_df(df, X_y=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3d1d7b",
      "metadata": {
        "id": "ab3d1d7b",
        "outputId": "97a49ce5-4e57-4a04-f626-205e0d25e98a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Instances:\t96\n",
            "--\n",
            "Outer Train set:\t87\n",
            "Test set:\t\t9\n",
            "--\n",
            "Inner Train set:\t70\n",
            "Validation set:\t\t17\n",
            "\n"
          ]
        }
      ],
      "source": [
        "FITNESS_FUNCTION = 'rmse'\n",
        "MINIMIZATION = True\n",
        "\n",
        "total_instances = X.shape[0]\n",
        "outer_test_size = total_instances // k_outer\n",
        "outer_train_size = total_instances - outer_test_size\n",
        "inner_val_size = outer_train_size // k_inner\n",
        "inner_train_size = outer_train_size - inner_val_size\n",
        "\n",
        "print(f'Total Instances:\\t{total_instances}\\n--')\n",
        "print(f'Outer Train set:\\t{outer_train_size}')\n",
        "print(f'Test set:\\t\\t{outer_test_size}\\n--')\n",
        "print(f'Inner Train set:\\t{inner_train_size}')\n",
        "print(f'Validation set:\\t\\t{inner_val_size}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fce2f16",
      "metadata": {
        "id": "9fce2f16"
      },
      "outputs": [],
      "source": [
        "POP_SIZE = 20\n",
        "\n",
        "fixed_params = {\n",
        "    # ---\n",
        "    # Search Space\n",
        "    'initializer': 'rhh',\n",
        "    'init_depth': 2,\n",
        "    'tree_constants': [random.uniform(-1, 1) for _ in range(9)],\n",
        "    'tree_functions': ['add', 'subtract','multiply','divide'],\n",
        "    #'prob_const': 0.1,\n",
        "    # ---\n",
        "    # Problem Instance\n",
        "    'dataset_name': DATASET_NAME,\n",
        "    'fitness_function': 'rmse',\n",
        "    'minimization': True,\n",
        "    # ---\n",
        "    # Model instance\n",
        "    #'tournament_size': int(POP_SIZE*0.02) if POP_SIZE>100 else 2,\n",
        "    'pop_size': POP_SIZE,\n",
        "    # ---\n",
        "    # Solve settings\n",
        "    'n_iter': 500,\n",
        "    'elitism': True,\n",
        "    'n_elites': 2,\n",
        "    'test_elite': True,\n",
        "    'log_level': LOG_LEVEL,\n",
        "    'verbose': 0,\n",
        "    'n_jobs': 1,\n",
        "\n",
        "    # ---\n",
        "    # GP unique settings\n",
        "    #'max_depth': 10,\n",
        "    #'p_xo' : 0.5,\n",
        "\n",
        "    # ---\n",
        "    # GSGP unique settings\n",
        "    #'ms_lower': 0,\n",
        "    #'ms_upper': 0.5,\n",
        "    # ---\n",
        "    # SLIM unique settings\n",
        "    #'max_depth': 10,\n",
        "    #'#p_inflate': 0.70,\n",
        "    #'slim_version': 'SLIM+SIG2',\n",
        "    #'copy_parent': True,\n",
        "    #'ms_lower': 0,\n",
        "    #'ms_upper': 0.5,\n",
        "    #'reconstruct': False,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "#SLIM_VERSIONS = ['SLIM+SIG2', 'SLIM+SIG1', 'SLIM+ABS', 'SLIM*SIG2', 'SLIM*SIG1', 'SLIM*ABS']\n",
        "\n",
        "param_grid = {\n",
        "        'p_xo' : [0.5, 0.7],\n",
        "        'tournament_size': [int(POP_SIZE*0.20), int(POP_SIZE*0.10), int(POP_SIZE*0.15)],\n",
        "        'prob_const': [0.1, 0.7],\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLI4qOnwaqEh",
        "outputId": "231a67f4-f484-4211-e2cd-10b16172032b"
      },
      "id": "RLI4qOnwaqEh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1000712146279652,\n",
              " -0.26376772693124484,\n",
              " -0.7278541834173331,\n",
              " -0.7535167763157982,\n",
              " 0.2480275455391745,\n",
              " 0.44235145919545893,\n",
              " -0.3546038501880595,\n",
              " -0.4598648657688593,\n",
              " 0.05518407112117796]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b10a47",
      "metadata": {
        "id": "f5b10a47"
      },
      "outputs": [],
      "source": [
        "def gp_nested_cross_validation(X, y, gp_model,  k_outer, k_inner, fixed_params, param_grid, seed, LOG_DIR, DATASET_NAME):\n",
        "    \"\"\"\n",
        "    Perform nested cross-validation for a given model and dataset.\n",
        "\n",
        "    Args:\n",
        "        X (torch.Tensor): Feature matrix.\n",
        "        y (torch.Tensor): Target vector.\n",
        "        gp_model (callable): The gp model to be evaluated.\n",
        "        k_outer (int): Number of outer folds.\n",
        "        k_inner (int): Number of inner folds.\n",
        "        fixed_params (dict): Fixed parameters for the model.\n",
        "        param_grid (dict): Parameter grid for hyperparameter tuning.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing model results.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    cv_outer = KFold(n_splits=k_outer, random_state=seed, shuffle=True)\n",
        "    cv_inner = KFold(n_splits=k_inner, random_state=seed, shuffle=True)\n",
        "\n",
        "    data_cv_outer = [[learning_ix, test_ix] for learning_ix, test_ix in cv_outer.split(X, y)]\n",
        "\n",
        "    models = []\n",
        "\n",
        "    for i, (train_ix, test_ix) in enumerate(data_cv_outer):\n",
        "        print(f'Outer fold {i+1}/{k_outer}')\n",
        "        X_learning, y_learning = X[train_ix], y[train_ix]\n",
        "        X_test, y_test = X[test_ix], y[test_ix]\n",
        "\n",
        "        # Inner cross-validation\n",
        "        results = []\n",
        "\n",
        "        data_cv_inner = [[learning_ix, val_ix] for learning_ix, val_ix in cv_inner.split(X_learning, y_learning)]\n",
        "        for j, (train_ix, val_ix) in enumerate(data_cv_inner):\n",
        "\n",
        "            # Split the data into training and validation sets K times\n",
        "            print(f'-----\\n Inner fold {j+1}/{k_inner}')\n",
        "            X_inner_train, y_inner_train = X_learning[train_ix], y_learning[train_ix]\n",
        "            X_inner_val, y_inner_val = X_learning[val_ix], y_learning[val_ix]\n",
        "\n",
        "            print(f'Training shape: {X_inner_train.shape}\\nValidation shape: {X_inner_val.shape}\\n')\n",
        "\n",
        "            # Update the X and y values in the fixed_params dictionary\n",
        "            fixed_params.update({\n",
        "                'X_train': X_inner_train, 'y_train': y_inner_train,\n",
        "                'X_test': X_inner_val, 'y_test': y_inner_val\n",
        "            })\n",
        "\n",
        "            # Update LOG_PATH in the fixed_params dictionary\n",
        "            LOG_PATH = LOG_DIR+DATASET_NAME+''+'outer'+''+str(i)+''+'inner'+'_'+str(j)+'.csv'\n",
        "            if os.path.exists(LOG_PATH):\n",
        "                os.remove(LOG_PATH)\n",
        "            fixed_params.update({'log_path': LOG_PATH})\n",
        "\n",
        "\n",
        "            res = fit_model_GridSearch(gp_model=gp_model, fixed_params=fixed_params, param_grid=param_grid, seed=(seed+k_inner))\n",
        "\n",
        "            # Log\n",
        "            results.append(res)\n",
        "\n",
        "        medians = group_and_median_rmse(results)\n",
        "\n",
        "        # Find minimum median rmse\n",
        "        best_dynamic_combo_median = min(medians, key=lambda x: x['rmse_test_median'])\n",
        "\n",
        "        print(f'Best inner combination: {best_dynamic_combo_median[\"dynamic_params\"]} with median RMSE: {best_dynamic_combo_median[\"rmse_test_median\"]}')\n",
        "\n",
        "        # Train the best model on the entire training set\n",
        "        print('Training best combination on entire learning set')\n",
        "\n",
        "        best_hyper_combo = best_dynamic_combo_median['dynamic_params']\n",
        "\n",
        "        fixed_params.update({\n",
        "                'X_train': X_learning, 'y_train': y_learning,\n",
        "                'X_test': X_test, 'y_test': y_test\n",
        "            })\n",
        "\n",
        "        LOG_PATH = LOG_DIR+DATASET_NAME+'_'+'outer'+'_'+str(i)+'.csv'\n",
        "        if os.path.exists(LOG_PATH):\n",
        "            os.remove(LOG_PATH)\n",
        "        fixed_params.update({'log_path': LOG_PATH})\n",
        "\n",
        "        full_params = {**fixed_params, **best_hyper_combo}\n",
        "\n",
        "        outer_model = gp_model(**full_params, seed=(seed+k_outer))\n",
        "\n",
        "        res = {'model': outer_model}\n",
        "        res.update({'rmse_train': outer_model.fitness.item()})\n",
        "        res.update({'rmse_test': outer_model.test_fitness.item()})\n",
        "        res.update({'dynamic_params': best_hyper_combo})\n",
        "\n",
        "        models.append(res)\n",
        "\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07ded42",
      "metadata": {
        "id": "c07ded42",
        "outputId": "13f00ac0-526e-4fd1-ce5f-e6c041c0774c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outer fold 1/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([68, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7} with median RMSE: 1.97232985496521\n",
            "Training best combination on entire learning set\n",
            "Outer fold 2/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([68, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.7} with median RMSE: 3.622127056121826\n",
            "Training best combination on entire learning set\n",
            "Outer fold 3/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([68, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7} with median RMSE: 2.244109630584717\n",
            "Training best combination on entire learning set\n",
            "Outer fold 4/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([68, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.1} with median RMSE: 3.360670328140259\n",
            "Training best combination on entire learning set\n",
            "Outer fold 5/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([68, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7} with median RMSE: 3.4664435386657715\n",
            "Training best combination on entire learning set\n",
            "Outer fold 6/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([68, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7} with median RMSE: 1.4730985164642334\n",
            "Training best combination on entire learning set\n",
            "Outer fold 7/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7} with median RMSE: 1.727198600769043\n",
            "Training best combination on entire learning set\n",
            "Outer fold 8/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.1} with median RMSE: 3.9114387035369873\n",
            "Training best combination on entire learning set\n",
            "Outer fold 9/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7} with median RMSE: 3.592029333114624\n",
            "Training best combination on entire learning set\n",
            "Outer fold 10/10\n",
            "-----\n",
            " Inner fold 1/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 2/5\n",
            "Training shape: torch.Size([69, 12])\n",
            "Validation shape: torch.Size([18, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 3/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 4/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "-----\n",
            " Inner fold 5/5\n",
            "Training shape: torch.Size([70, 12])\n",
            "Validation shape: torch.Size([17, 12])\n",
            "\n",
            "Best inner combination: {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.1} with median RMSE: 3.3674376010894775\n",
            "Training best combination on entire learning set\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654ddf7d0>,\n",
              "  'rmse_train': 4.808901309967041,\n",
              "  'rmse_test': 6.494765758514404,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654f2c310>,\n",
              "  'rmse_train': 2.954871654510498,\n",
              "  'rmse_test': 1.7341370582580566,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654f4e250>,\n",
              "  'rmse_train': 10.69654369354248,\n",
              "  'rmse_test': 8.891355514526367,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654df38d0>,\n",
              "  'rmse_train': 3.7210168838500977,\n",
              "  'rmse_test': 2.7273669242858887,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.1}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654de2690>,\n",
              "  'rmse_train': 10.813080787658691,\n",
              "  'rmse_test': 8.572257995605469,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654bc7e10>,\n",
              "  'rmse_train': 7.404740810394287,\n",
              "  'rmse_test': 6.907686710357666,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654cd3fd0>,\n",
              "  'rmse_train': 10.189078330993652,\n",
              "  'rmse_test': 12.933025360107422,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2654e037d0>,\n",
              "  'rmse_train': 3.0783557891845703,\n",
              "  'rmse_test': 3.3322343826293945,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.1}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a2655bb2850>,\n",
              "  'rmse_train': 5.6223320960998535,\n",
              "  'rmse_test': 3.8437631130218506,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 4, 'prob_const': 0.7}},\n",
              " {'model': <slim_gsgp.algorithms.GP.representations.tree.Tree at 0x7a265654fa90>,\n",
              "  'rmse_train': 3.5312254428863525,\n",
              "  'rmse_test': 3.763768434524536,\n",
              "  'dynamic_params': {'p_xo': 0.5, 'tournament_size': 2, 'prob_const': 0.1}}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "gp_nested_cross_validation(X, y, gp_model=gp, k_outer=k_outer, k_inner=k_inner, fixed_params=fixed_params, param_grid=param_grid, seed=seed, LOG_DIR=LOG_DIR, DATASET_NAME=DATASET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r8-dVl5ak6V",
        "outputId": "3c30b2cd-54ad-416d-e559-921eeac409ee"
      },
      "id": "-r8-dVl5ak6V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data\t\t log\t    performance_metrics  tests\n",
            "gp_grid_log.csv  Notebooks  requirements.txt\t utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5576c88f",
      "metadata": {
        "id": "5576c88f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44f5faf-e05b-461b-f0f5-ab312851a16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: log/ (stored 0%)\n",
            "  adding: log/gp.csv (deflated 89%)\n",
            "  adding: log/gp_settings.csv (deflated 94%)\n",
            "  adding: log/GP/ (stored 0%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_0.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_8.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_1.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_0.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_0.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_8_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_3_settings.csv (deflated 64%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_0.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_3.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_0.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_0.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_0_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_3.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_3.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_3.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_1.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_0.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_0.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_0.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_4.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_1.csv (deflated 83%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_1_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_6_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_0.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_2.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_5.csv (deflated 83%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_2.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_9_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_2_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_4.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_6.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_4_settings.csv (deflated 64%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_7.csv (deflated 83%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_4.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_3.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter0inner_3.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_3.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_5_settings.csv (deflated 64%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_0.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter1inner_2.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_3_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_3.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_3.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_4.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_1.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_2.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_4.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter9inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_4_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_3.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_1.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeedouter7inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_9.csv (deflated 83%)\n",
            "  adding: log/GP/GP_sustavianfeedouter8inner_1_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter2inner_2.csv (deflated 85%)\n",
            "  adding: log/GP/GP_sustavianfeed_outer_7_settings.csv (deflated 65%)\n",
            "  adding: log/GP/GP_sustavianfeedouter6inner_0_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter5inner_3.csv (deflated 84%)\n",
            "  adding: log/GP/GP_sustavianfeedouter3inner_2_settings.csv (deflated 93%)\n",
            "  adding: log/GP/GP_sustavianfeedouter4inner_3_settings.csv (deflated 93%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r logs.zip log/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ffd5a2f",
      "metadata": {
        "id": "7ffd5a2f"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be44f6d",
      "metadata": {
        "id": "8be44f6d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "from plotly.subplots import make_subplots\n",
        "import ast\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "addb8465",
      "metadata": {
        "id": "addb8465"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../log/gp.csv\", header=None)\n",
        "settings_df = pd.read_csv(\"../log/gp_settings.csv\", header=None)\n",
        "unique_settings_df = settings_df.drop_duplicates(0) #the comb id is now indexed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df47a03",
      "metadata": {
        "id": "3df47a03"
      },
      "outputs": [],
      "source": [
        "unique_settings_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f85f5c8",
      "metadata": {
        "id": "7f85f5c8"
      },
      "outputs": [],
      "source": [
        "# 0  - Algorithm\n",
        "# 1  - Instance ID\n",
        "# 2  - Dataset\n",
        "# 3  - Seed\n",
        "# 4  - Generation\n",
        "# 5  - Fitness\n",
        "# 6  - Running time\n",
        "# 7  - Population nodes\n",
        "# 8  - Test fitness\n",
        "# 9  - Elite nodes\n",
        "# 10 - niche entropy\n",
        "\"\"\"From here on, it doesnt appear on df\"\"\"\n",
        "# 11 - sd(pop.fit)\n",
        "# 12 - Log level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb68776",
      "metadata": {
        "id": "8eb68776"
      },
      "outputs": [],
      "source": [
        "'''def get_combination_str(setting_str, unique_settings_df = pd.DataFrame(df[1].unique())):\n",
        "    comb_str = unique_settings_df[unique_settings_df[0]==setting_str][1][0]\n",
        "    return comb_str'''\n",
        "\n",
        "def param_in_combination(param:str, comb_str: str):\n",
        "    \"\"\"\n",
        "    Parameters that the function can receive:\n",
        "    'log' / 'verbose'/ 'test_elite' / 'n_jobs' / 'max_depth' / 'n_elites' / 'elistism' / 'n_iter'\n",
        "    'settings_dict' / 'p_xo' / 'pop_size' / 'seed' / 'p_m' / 'p_c' / 'init_depth' / 'init_pop_size'\n",
        "    \"\"\"\n",
        "    variable_init = comb_str.find(param)\n",
        "    param_init = variable_init + len(param) + 3 #advance 3 steps to account for the quote, the dots and the space.\n",
        "    param_end = comb_str.find(',', variable_init)\n",
        "    param_value = comb_str[param_init:param_end]\n",
        "    if param_value.startswith('<') or param_value.startswith('['):\n",
        "        print('The value of the parameter given cannot be converted to its original class:')\n",
        "    else:\n",
        "        try:\n",
        "            return ast.literal_eval(param_value)\n",
        "        except SyntaxError as s:\n",
        "            print(f'Parameter does not exist in the combination string or it cannot be accessed. ({s})')\n",
        "\n",
        "#example\n",
        "#print(get_combination_str('865732c8-3014-11f0-b37b-baa0ecd080fe'))\n",
        "#print(param_in_combination('log', get_combination_str('865732c8-3014-11f0-b37b-baa0ecd080fe')))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a37af0",
      "metadata": {
        "id": "96a37af0"
      },
      "outputs": [],
      "source": [
        "'''def pop_fitness_diversity(df, train_color='blue'):\n",
        "     \"\"\"\n",
        "     Out of Bounds\n",
        "     \"\"\"\n",
        "     dif_combs = np.unique(df[[1]])\n",
        "     for comb in dif_combs:\n",
        "          y = df[df[1]==comb]\n",
        "          #comb_dict = get_combination(comb)\n",
        "          fig = go.Figure()\n",
        "          fig.add_trace(go.Scatter(y=y.iloc[:,11].values,\n",
        "                                   mode='lines', name='Train', line=dict(color=train_color)))\n",
        "          fig.update_layout(\n",
        "          height=400, width=800,\n",
        "          margin=dict(t=50),\n",
        "          yaxis_range=[0,None],\n",
        "          title_text=f'GP - Population Fitness Diversity\\nCombination:',\n",
        "          xaxis_title='Generation', yaxis_title='Fitness Standard Deviation'\n",
        "          )\n",
        "          fig.show()'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a45840",
      "metadata": {
        "id": "d2a45840"
      },
      "outputs": [],
      "source": [
        "def train_test_fit(df, train_color='blue', test_color='orange', rows=5, cols=4):\n",
        "    dif_combs = df[1].unique()  # Get unique combinations\n",
        "    unique_setting_df = pd.DataFrame(dif_combs)\n",
        "    num_plots = len(dif_combs)\n",
        "    assert rows*cols==num_plots\n",
        "\n",
        "    # Create subplot grid\n",
        "    fig = sp.make_subplots(rows=rows, cols=cols,\n",
        "                           subplot_titles=[f\"Combination index: {unique_settings_df[unique_settings_df[0]==comb].index[0]}\"\n",
        "                                           for comb in dif_combs])\n",
        "\n",
        "    for i, comb in enumerate(dif_combs):\n",
        "        y = df[df[1] == comb]\n",
        "        algo = y.iloc[0,0]\n",
        "        row = (i // cols) + 1  #Calculate row position\n",
        "        col = (i % cols) + 1   #Calculate column position\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(y=y.iloc[:, 5].values, mode='lines', name='Train', line=dict(color=train_color),\n",
        "                       showlegend=(i==0)),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(y=y.iloc[:, 8].values, mode='lines', name='Test', line=dict(color=test_color),\n",
        "                       showlegend=(i==0)),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "        fig.update_yaxes(range=[0, None], row=row, col=col)\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=150 * rows,\n",
        "        width=250 * cols,\n",
        "        margin=dict(t=50),\n",
        "        title_text=f'{algo} - Train vs Test Fitness (x=Generation, y=RMSE)',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    fig.update_annotations(font_size=10)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a375f5",
      "metadata": {
        "id": "a1a375f5"
      },
      "outputs": [],
      "source": [
        "def train_test_fit_and_size(df, comb_idxs: list | int = [i for i in range(pd.DataFrame(df[1].unique()).shape[0])],\n",
        "                            train_color='blue', test_color='orange'):\n",
        "     unique_setting_df = pd.DataFrame(df[1].unique())\n",
        "     for comb_idx in comb_idxs:\n",
        "          comb = unique_settings_df.iloc[comb_idx, 0]\n",
        "          y = df[df[1]==comb]\n",
        "          algo = y.iloc[0,0]\n",
        "          fig = make_subplots(\n",
        "          rows=1, cols=2,\n",
        "          subplot_titles=(f'{algo} - Fitness evolution\\nCombination:', f'{algo} - Size evolution')\n",
        "          )\n",
        "\n",
        "          fig.add_trace(go.Scatter(y=y.iloc[:,5].values,\n",
        "                                   mode='lines', name='Train', line=dict(color=train_color)), row=1, col=1)\n",
        "          fig.add_trace(go.Scatter(y=y.iloc[:,8].values,\n",
        "                                   mode='lines', name='Test', line=dict(color=test_color)), row=1, col=1)\n",
        "          fig.add_trace(go.Scatter(y=y.iloc[:,9].values,\n",
        "                                   mode='lines', name='Size'), row=1, col=2)\n",
        "\n",
        "          fig.update_xaxes(title_text=\"Generation\")\n",
        "\n",
        "          fig.update_layout(\n",
        "          width=1000,\n",
        "          height=400,\n",
        "          showlegend=True,\n",
        "          yaxis_range=[0,None],\n",
        "          )\n",
        "          fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bfe576",
      "metadata": {
        "id": "f6bfe576"
      },
      "outputs": [],
      "source": [
        "def niche_entropy(df, train_color='blue', rows=5, cols=4):\n",
        "    dif_combs = df[1].unique()  # Get unique combinations\n",
        "    unique_setting_df = pd.DataFrame(dif_combs) # array to df\n",
        "    num_plots = len(dif_combs)\n",
        "    assert rows*cols==num_plots, \"The number of combinations does not correspond to the grid size defined (rows/cols).\"\n",
        "\n",
        "    fig = sp.make_subplots(rows=rows, cols=cols,\n",
        "                           subplot_titles=[f\"Combination index: {unique_settings_df[unique_settings_df[0]==comb].index[0]}\"\n",
        "                                           for comb in dif_combs])\n",
        "\n",
        "    for i, comb in enumerate(dif_combs):\n",
        "        y = df[df[1] == comb]\n",
        "        algo = y.iloc[0,0]\n",
        "        row = (i // cols) + 1\n",
        "        col = (i % cols) + 1\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                y=y.iloc[:, 10].values,\n",
        "                mode='lines',\n",
        "                name='Niche Entropy',\n",
        "                line=dict(color=train_color),\n",
        "                showlegend=(i == 0)), row=row, col=col\n",
        "                )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=150 * rows,\n",
        "        width=250 * cols,\n",
        "        margin=dict(t=50),\n",
        "        title_text=f'{algo} - Niche Entropy (x=Generation, y=Entropy)',\n",
        "    )\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe9fcf3",
      "metadata": {
        "id": "abe9fcf3"
      },
      "outputs": [],
      "source": [
        "def plot_combs_together_test(df, comb_idxs: list | int = [i for i in range(pd.DataFrame(df[1].unique()).shape[0])],\n",
        "                             colors = ['#FF0000', '#0000FF', '#00FF00', '#FFA500', '#800080',\n",
        "                                       '#FF00FF', '#00FFFF', '#FFFF00', '#1F77B4', '#FF7F0E',\n",
        "                                       '#2CA02C', '#D62728', '#9467BD', '#8C564B', '#E377C2',\n",
        "                                       '#7F7F7F', '#AEC7E8', '#FFBB78', '#98DF8A', '#FF9896'],\n",
        "                              ):\n",
        "\n",
        "     assert len(colors)>=len(comb_idxs), \"Not enough colors for all combinations\"\n",
        "\n",
        "     unique_settings_df = pd.DataFrame(df[1].unique())\n",
        "     fig = go.Figure()\n",
        "     for i, comb_idx in enumerate(comb_idxs):\n",
        "          comb = unique_settings_df.iloc[comb_idx, 0]\n",
        "          y = df[df[1]==comb]\n",
        "          algo = y.iloc[0,0]\n",
        "\n",
        "          fig.add_trace(go.Scatter(y=y.iloc[:,8].values,\n",
        "                                   mode='lines', name=f'Test Comb {comb_idx}',\n",
        "                                   line=dict(color=colors[i])))#, row=1, col=1)\n",
        "\n",
        "          fig.update_xaxes(title_text=\"Generation\")\n",
        "\n",
        "     fig.update_layout(\n",
        "          width=1000,\n",
        "          height=400,\n",
        "          title_text = f\"{algo} - Test Fitness (Combinations indexes: {comb_idxs})\",\n",
        "          showlegend=True,\n",
        "          yaxis_range=[0,None],\n",
        "          )\n",
        "\n",
        "     fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98572ec",
      "metadata": {
        "id": "f98572ec"
      },
      "outputs": [],
      "source": [
        "train_test_fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9910b00",
      "metadata": {
        "id": "f9910b00"
      },
      "outputs": [],
      "source": [
        "train_test_fit_and_size(df, comb_idxs=[3,19])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e1ada2",
      "metadata": {
        "id": "d7e1ada2"
      },
      "outputs": [],
      "source": [
        "niche_entropy(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552e88f3",
      "metadata": {
        "id": "552e88f3"
      },
      "outputs": [],
      "source": [
        "plot_combs_together_test(df,comb_idxs=[1,2,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f16023f",
      "metadata": {
        "id": "7f16023f"
      },
      "source": [
        "Modular functions for different versions/hyperparameters combinations\n",
        "ASSUMING THAT:\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed39a663",
      "metadata": {
        "id": "ed39a663"
      },
      "outputs": [],
      "source": [
        "SLIM_VERSIONS = ['SLIM+SIG2', 'SLIM+SIG1', 'SLIM+ABS', 'SLIM*SIG2', 'SLIM*SIG1', 'SLIM*ABS']\n",
        "COMBINATIONS = [ for i in df[1].unique()]\n",
        "\n",
        "\"\"\"param_grid = {\n",
        "    'slim_version': SLIM_VERSIONS\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390510b3",
      "metadata": {
        "id": "390510b3"
      },
      "outputs": [],
      "source": [
        "rmse_by_config = defaultdict(list)\n",
        "\n",
        "for split in results:\n",
        "    rmse_train = []\n",
        "    rmse_test = []\n",
        "\n",
        "    for result in split:\n",
        "        key = ''\n",
        "        for k, v in result['dynamic_params'].items():\n",
        "            key += k+': '+str(v)+' <br /> '\n",
        "        rmse_by_config[key].append(result['rmse_test'])\n",
        "\n",
        "fig = go.Figure()\n",
        "for config, rmse_values in rmse_by_config.items():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=rmse_values,\n",
        "        boxpoints='all',\n",
        "        jitter=0.5,\n",
        "        pointpos=0,\n",
        "        line=dict(color='orange'),\n",
        "        name=config\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=DATASET_NAME+' dataset',\n",
        "    xaxis_title='',\n",
        "    yaxis_title='Test RMSE',\n",
        "    height=500, width=1100,\n",
        "    xaxis_tickangle=-90,\n",
        "    yaxis_range=[0,None],\n",
        "    margin=dict(l=50, r=50, t=50, b=20),\n",
        "    showlegend=False,\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd428207",
      "metadata": {
        "id": "bd428207"
      },
      "outputs": [],
      "source": [
        "# Plot settings\n",
        "df_log = []\n",
        "for i_inner in range(k_inner):\n",
        "    tmp = pd.read_csv(LOG_DIR+'slim_'+DATASET_NAME+'_'+str(i_inner)+'.csv', header=None)\n",
        "    tmp['cv'] = i_inner\n",
        "    df_log.append(tmp)\n",
        "df_log = pd.concat(df_log, ignore_index=True)\n",
        "\n",
        "n_rows = 2\n",
        "n_cols = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95d332a",
      "metadata": {
        "id": "c95d332a"
      },
      "outputs": [],
      "source": [
        "make_evolution_plots(n_rows, n_cols, SLIM_VERSIONS, df_log,\n",
        "                     plot_title = 'SLIM - Train vs Test Fitness ('+DATASET_NAME+' dataset)')\n",
        "[fixed_params['pop_size'], fixed_params['tournament_size']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a946d562",
      "metadata": {
        "id": "a946d562"
      },
      "outputs": [],
      "source": [
        "make_evolution_plots(n_rows, n_cols, SLIM_VERSIONS, df_log, var='size'\n",
        "                     plot_title = 'SLIM -Size ('+DATASET_NAME+' dataset)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}