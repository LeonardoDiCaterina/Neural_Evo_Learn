{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Networks - Learning and Implementation strategies\n",
    "\n",
    "In this notebook, we will study the use of `PyTorch` and `Tensorflow` frameworks for implementing and training Neural Networks. This is not intended to be exhaustive, but rather to provide examples for exploring the algorithms and their hyperparameters with these frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR (exclusive OR) data simulation\n",
    "\n",
    "Let's run our examples for a very simple non-linear binary classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes\n",
      "[torch.Size([150, 2]), torch.Size([150])] [torch.float32, torch.float32] [device(type='cpu'), device(type='cpu')]\n",
      "[torch.Size([50, 2]), torch.Size([50])] [torch.float32, torch.float32] [device(type='cpu'), device(type='cpu')]\n",
      "\n",
      "Batch sizes\n",
      "150\n",
      "1\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Data simulation\n",
    "N_data = 200\n",
    "train_size = 150\n",
    "X = 2 * torch.rand(N_data, 2, device=device, dtype=torch.float32) - 1\n",
    "y = torch.tensor([0 if elem[0]*elem[1] < 0 else 1 for elem in X], device=device, dtype=torch.float32)\n",
    "\n",
    "# Split training and test partitions\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:, :]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "# Define datasets for data loaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size_GD = train_size\n",
    "train_dl_GD = DataLoader(train_ds, batch_size_GD, shuffle=True)\n",
    "val_dl_GD = DataLoader(val_ds, batch_size_GD, shuffle=True)\n",
    "\n",
    "batch_size_SGD = 1\n",
    "train_dl_SGD = DataLoader(train_ds, batch_size_SGD, shuffle=True)\n",
    "val_dl_SGD = DataLoader(val_ds, batch_size_SGD, shuffle=True)\n",
    "\n",
    "batch_size_MiniSGD = 32\n",
    "train_dl_MniSGD = DataLoader(train_ds, batch_size_MiniSGD, shuffle=True)\n",
    "val_dl_MiniSGD = DataLoader(val_ds, batch_size_MiniSGD, shuffle=True)\n",
    "\n",
    "print('Partition sizes')\n",
    "print([X_train.shape, y_train.shape], [X_train.dtype, y_train.dtype], [X_train.device, y_train.device])\n",
    "print([X_val.shape, y_val.shape], [X_val.dtype, y_val.dtype], [X_val.device, y_val.device])\n",
    "\n",
    "print('\\nBatch sizes')\n",
    "print(batch_size_GD)\n",
    "print(batch_size_SGD)\n",
    "print(batch_size_MiniSGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.35,
          "size": 9
         },
         "mode": "markers",
         "name": "Class 0",
         "type": "scatter",
         "x": {
          "bdata": "RikTv0CIaL++2U+/uFayPmA73r2Ur7U+wvZFP3Ae/76MzBE/+F8dvtA3qT1+Ayy/tu9VPyyl+z5k56s+DEKovmidWT+Yyqc+RPClvljmcb6KoRc/cv1Sv7bSXT9wYfg9zCt1P/atL79g/6297O7NPnTxOj8QC0g++JXXPiAfYb3sMr0+xCNnv555Az+USoS+bEEuPxggtz4Q0QY+boMLv+bIYj+QtlA/6PtgPqCGsz0yXXQ/Eg4OP0jXI75wD3A+xhheP/CvVD6mb3I/jIDOvlR+kL5YvQY+kF6LPdLCCT9STE8/TNFdv0AYvb1yyW6/oGsAP+CvCD04Eyc/RArmviCVXz3Qqbw+4N/SPY4Ufr8ApuA7lHmxPgCGV7zQTY6+yM5KPzigTD/kbtU+GhsmvwCbQL9cyky/Vq9wP94uB78oYxQ+EMrqPQZpUr9gVXs9VMySvhg9jz4QOya/vIRCv/D0oL2sFNO+Blddv4ivzb44xDw+1G35vqR9Oz9I7D0+6DIZPyi2QL/wT1A/zBBsv0DGeb7mFTS/wAk/PqA+CL+g+iw9ZD4sP3C2Q78=",
          "dtype": "f4"
         },
         "y": {
          "bdata": "rEJeP+iOXT8AOP871Pwpv0RCoj4gyzS+vHZxvzgP2T7IWOS+sjV7P0hLFL/GV3Y/pOkAvxwUyL4akTu/YGy7PSKbRr9Y4V2/HkM6P1xLnj48RCe/XEXEPpC7br+45Kq+YEr1vrqCPz+EbKQ+tgouv97sUL/Aj/C9QJWGvqgmrT5w8Y697gwdPxYYGr88FF8/+Ao+vmr/cb/gM/29fB9hP1Acq70Y8W6+CvdZv+DFT7+g80y/EJ6LvZZFRj88Qpe+kIyWvfRJgr4QDJy+mghLPzC4xD1ofGe+UkUBvxhzBr+kPTS/oPS+PhRNxz7I1Dg+HAsBvyBXBr2OUhu/TGTWPtR69L74682+ePmNvnBsNj6inCC/nB+HvqBfyT78VEo/hJzCvuIAWr9QvD2++FLVPqCocD4AdhE9gO8ovybHZT9Ufbu+omQxv1xoqz7cJtu+gId5PaQ62b4yICg/gGZPP/D6RT9gCck+HiBfP5h/4T6Oj3e/sIjFPhb8Wr/M0Q6/APEiv6ATED/moFG/gAJEPAI9UT+gtDg+aDc1v8B6KT0Ek/m+mqURv+Ao5D4=",
          "dtype": "f4"
         }
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.35,
          "size": 9
         },
         "mode": "markers",
         "name": "Class 1",
         "type": "scatter",
         "x": {
          "bdata": "EAKhvjhLXr6wcGs+Opl3P65TGr+wg829koIxPwgy774GzzI/nq5dP7R7ZL8c4Jc+JNbgPgQLZb9I9OW+wg5+v+zetj4QAaq+0NN9PoC5sr66mjA/smZgP7wDbr9us2A/er1aP5Qqo77cvRs/wCBzPcYjWb8w3bw+aCR1PrCQhz0EQWm//jcMv8z28D4gc/K+HOs9v3DDSj6C3VE/EAxQv7QGq76eKE2/qAnQPqiIn77ACiw9fDVKv1KhXj/gkiE+OPbuvga0Zz8ARmk+0B4OPs7OeL9QC0c+2GiZvmBXdr0aMGo/5OmZPgp+Pz/K23Y/XtUHvzA7kb7QeQq/kJ5bPh5MGD8YalC+cDcrv8piJz9Q/CY/YGFQv2hxKz+AJae+EJDBvljFsj6YkLI+tKT1vkgWxD7YiFi+AE7PPvSiqT4AVEc9WEW1voAgIT5sgPE+7BP0PjAkP74Y9RC/kJ9DPqTR6j6sLpK+wNrOPKJRJD8IhFo/",
          "dtype": "f4"
         },
         "y": {
          "bdata": "4KZ3vugu474oj1I+MFKMPdRDl76ywW2/lkRRP5hos75C2CA/ADeFPXbpc7+gxog+PMaoPrSEo74Ejge/Ulgiv1RlRj+A1cO9FGxDPwA6H7voR/Q+wPwiP0wUWL8wDCo/RLuIPn52Hr84xUA+oDERPfRd1L7cfnc//lRZP1KZcT8ExEK/MDj9vSx6mj5+dEe/IPJuvaxebT8Yh/4+sJgKvpj9x75cm5m+FMkgPw51Q7+YHWI+tHR4v8y/pj5kPwM/PPbZvgAz0Dv094A+FP8nP+g6T79AFkk9AGuqu47+cL/ApMM9UCvSPpA/cD+EaPU+om4lv/peXr94pXS/6NoLPlxzkj4gosu+nME1v+ztuj5iNFs/WMi3vmJhPj+UMWS/3OyLvn6WVT9Ad4w84DcsvUaYbD8EDY6+fqUDP0CJ6T0IhGA/AA9dv6x70D746Wc+0JegPbBc8L4A/Gq+pLPLPnAzxz0m8ia/yNejPnyMgz5QwgQ+",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "x": 1,
         "y": 1
        },
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 20
        },
        "shapes": [
         {
          "line": {
           "color": "black",
           "dash": "dot"
          },
          "type": "line",
          "x0": 0,
          "x1": 0,
          "y0": -1.75,
          "y1": 1.75
         },
         {
          "line": {
           "color": "black",
           "dash": "dot"
          },
          "type": "line",
          "x0": -1.75,
          "x1": 1.75,
          "y0": 0,
          "y1": 0
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 550,
        "xaxis": {
         "range": [
          -1.25,
          1.25
         ],
         "title": {
          "text": "$x_1$"
         }
        },
        "yaxis": {
         "range": [
          -1.25,
          1.25
         ],
         "title": {
          "text": "$x_2$"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_np = X.cpu().numpy()\n",
    "y_np = y.cpu().numpy()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Class 0\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_np[y_np == 0, 0],\n",
    "    y=X_np[y_np == 0, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(color='blue', size=9, opacity=0.35),\n",
    "    name='Class 0'\n",
    "))\n",
    "\n",
    "# Class 1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_np[y_np == 1, 0],\n",
    "    y=X_np[y_np == 1, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=9, opacity=0.35),\n",
    "    name='Class 1'\n",
    "))\n",
    "\n",
    "fig.add_shape(type='line', x0=0, x1=0, y0=-1.75, y1=1.75, line=dict(color='black', dash='dot'))\n",
    "fig.add_shape(type='line', x0=-1.75, x1=1.75, y0=0, y1=0, line=dict(color='black', dash='dot'))\n",
    "fig.update_layout(\n",
    "    width=550,\n",
    "    height=400,\n",
    "    xaxis=dict(range=[-1.25, 1.25], title=r'$x_1$'),\n",
    "    yaxis=dict(range=[-1.25, 1.25], title=r'$x_2$'),\n",
    "    legend=dict(x=1, y=1),\n",
    "    margin=dict(l=20, r=20, t=20, b=20)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<hr />\n",
    "\n",
    "## 1. Building a Network with PyTorch\n",
    "\n",
    "Check the <a href='https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module'>PyTorch documentation</a> for details.\n",
    "\n",
    "To build a Neural Network in PyTorch, we create new Classes that inherit from the `nn.Module` class.\n",
    "\n",
    "This, some functionalities are already implemented. However, the model definitions have to be made mainly by hand. As a general rule, following the setps:\n",
    "\n",
    "1. Define the architecture of the network;\n",
    "2. Initialize the weights and biases of the network;\n",
    "3. Define the forward pass of the network;\n",
    "4. Define the training loop behaviour;\n",
    "5. Create the NN instance;\n",
    "6. Define the loss function;\n",
    "7. Define the optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 1. Network architecture\n",
    "\n",
    "Let's create a NN with three **linear layers**:\n",
    "\n",
    "1. The first will upscale the 2 input layer nodes (2 features in the dataset) to 3 nodes.\n",
    "2. The second will upscale to 4 nodes.\n",
    "3. The third (output) will, then, downscale to 1 node.\n",
    "\n",
    "Besides that, the **activation functions** should be:\n",
    "\n",
    "1. The ReLU function in the hidden layer, and\n",
    "2. The sigmoid function in the output layer.\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Instantiate the parent (nn.Module) class\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "        # NN architecture definition\n",
    "        ...\n",
    "\n",
    "```\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Weights and bias initialization\n",
    "\n",
    "For the weights and bias initialization, let's use methods that are already available in the PyTorch library. However, notice that you can easily implement your own initialization methods.\n",
    "\n",
    "- Weights: let's use the Xavier Uniform Initialization method. It maintains the variance of the activations remain the same across the layers of the network.\n",
    "- Bias: bias will be initializes with zeros.\n",
    "\n",
    "Take a look at the PyTorch docs for more available initialization methods:\n",
    "<a href='https://pytorch.org/docs/stable/nn.init.html'>https://pytorch.org/docs/stable/nn.init.html</a>.\n",
    "\n",
    "```python\n",
    "torch.nn.init.xavier_uniform_(attribute.weight)\n",
    "torch.nn.init.zeros_(attribute.bias)\n",
    "\n",
    "```\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 3. Forward pass\n",
    "\n",
    "This step sets how the linear combination of the inputs and weights of each layer should work and how the combination of the linear step should be combined with the activation functions:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # For each layer, the output will be the ReLu activation applied to the output of the linear operation\n",
    "    x = self.activation(self.fc1(x))\n",
    "    # For the last layer, the sigmoid function will be the activation\n",
    "    x = torch.sigmoid(self.fc2(x))\n",
    "    return x\n",
    "```\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 4. Training loop\n",
    "\n",
    "Having defined the Neural Network topology, the initialization method, and the feedforward pass, the behavior of the backpropagation should be set in the training loop.\n",
    "\n",
    "For that, Pytorch has some useful methods:\n",
    "\n",
    "- The `backward()` method calculates the derivative of the Error in respect to the NN weights applying the chain rule for hidden neurons;\n",
    "- The `step()` method updates the weights and bias based on the computed gradients.\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put steps 1 to 4 altogether:\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        #\n",
    "        # 1. 1. Network architecture\n",
    "        #\n",
    "        # Layer 1 (input to 3 nodes)\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        # Layer 2 (3 to 4 nodes)\n",
    "        self.fc2 = nn.Linear(3, 4)\n",
    "        # Layer 2 (4 nodes to 1 output node for binary classification)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        # Hidden layers activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        # Weights initialisation\n",
    "        # The apply method applies the function passed as the apply() argument\n",
    "        # to each element in the object, that in this case is the neural network.\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    #\n",
    "    # 1. 2. Weights and bias initialization\n",
    "    #\n",
    "    def _init_weights(self, attribute):\n",
    "        if isinstance(attribute, nn.Linear):\n",
    "          torch.nn.init.xavier_uniform_(attribute.weight)\n",
    "          torch.nn.init.zeros_(attribute.bias)\n",
    "    #\n",
    "    # 1. 3. Forward pass\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        # For each layer, the output will be the ReLu activation applied to the output of the linear operation\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # For the last layer, the sigmoid function will be the activation\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    #\n",
    "    # 1. 4. Training loop\n",
    "    # For details, see Machine Learning with PyTorch and Scikit-Learn.\n",
    "    #\n",
    "    def train(self, num_epochs, loss_fn, optimizer, train_dl, train_size, batch_size, x_valid, y_valid):\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "        # Loss and accuracy history objects initialization\n",
    "        loss_hist_train = [0] * num_epochs\n",
    "        accuracy_hist_train = [0] * num_epochs\n",
    "        loss_hist_valid = [0] * num_epochs\n",
    "        accuracy_hist_valid = [0] * num_epochs\n",
    "        \n",
    "        # Learning loop\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # Batch learn\n",
    "            for x_batch, y_batch in train_dl:\n",
    "                # ---\n",
    "                # 1.4.1. Get the predictions, the [:,0] reshapes from (batch_size,1) to (batch_size)\n",
    "                pred = self(x_batch)[:,0]\n",
    "                # 1.4.2. Compute the loss\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                # 1.4.3. Back propagate the gradients\n",
    "                # The `backward()` method, already available in PyTroch, calculates the \n",
    "                # derivative of the Error in respect to the NN weights\n",
    "                # applying the chain rule for hidden neurons.\n",
    "                loss.backward()\n",
    "                # 1.4.4. Update the weights based on the computed gradients\n",
    "                optimizer.step()\n",
    "                # ---\n",
    "                \n",
    "                # Reset to zero the gradients so they will not accumulate over the mini-batches\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update performance metrics\n",
    "                loss_hist_train[epoch] += loss.item()\n",
    "                is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "                accuracy_hist_train[epoch] += is_correct.mean()\n",
    "                \n",
    "            # Average the results\n",
    "            loss_hist_train[epoch] /= train_size/batch_size\n",
    "            accuracy_hist_train[epoch] /= train_size/batch_size\n",
    "            \n",
    "            # Predict the validation set\n",
    "            pred = self(x_valid)[:, 0]\n",
    "            loss_hist_valid[epoch] = loss_fn(pred, y_valid).item()\n",
    "            is_correct = ((pred>=0.5).float() == y_valid).float()\n",
    "            accuracy_hist_valid[epoch] += is_correct.mean()\n",
    "            \n",
    "        return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid\n",
    "\n",
    "    # Not needed normaly, it is just for mlextend plots\n",
    "    def predict(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        pred = self.forward(x)[:, 0]\n",
    "        return (pred>=0.5).float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_group_Y(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=2, output_size=1,\n",
    "                 hidden_layer_sizes=[3,4,5], activation=nn.ReLU()):\n",
    "        \n",
    "        super(Net_group_Y, self).__init__()\n",
    "        #\n",
    "        # 1. 1. Network architecture\n",
    "        \n",
    "        self.add_module(f'fc{1}', nn.Linear(input_size, hidden_layer_sizes[0]))\n",
    "        \n",
    "        for i in range(1,len(hidden_layer_sizes)):\n",
    "            self.add_module(f'fc{i+1}', nn.Linear(hidden_layer_sizes[i-1], hidden_layer_sizes[i]))\n",
    "        \n",
    "        self.add_module(f'fc{len(hidden_layer_sizes) +1 }', nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "\n",
    "        # Weights initialisation\n",
    "        # The apply method applies the function passed as the apply() argument\n",
    "        # to each element in the object, that in this case is the neural network.\n",
    "        self.apply(self._init_weights)\n",
    "        # Store the parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "    #\n",
    "    # 1. 2. Weights and bias initialization\n",
    "    #\n",
    "    def _init_weights(self, attribute):\n",
    "        if isinstance(attribute, nn.Linear):\n",
    "          torch.nn.init.xavier_uniform_(attribute.weight)\n",
    "          torch.nn.init.zeros_(attribute.bias)\n",
    "    #\n",
    "    # 1. 3. Forward pass\n",
    "    \"\"\"    \n",
    "    def forward(self, x):\n",
    "        # For each layer, the output will be the ReLu activation applied to the output of the linear operation\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # For the last layer, the sigmoid function will be the activation\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through all layers\n",
    "        for i in range(1, len(self.hidden_layer_sizes) + 2):\n",
    "            #print(f'forward pass layer {i}')\n",
    "            layer = getattr(self, f'fc{i}')\n",
    "            x = layer(x)\n",
    "            if i < len(self.hidden_layer_sizes):\n",
    "                x = self.activation(x)\n",
    "        # Apply sigmoid activation to the output layer\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    #\n",
    "    # 1. 4. Training loop\n",
    "    # For details, see Machine Learning with PyTorch and Scikit-Learn.\n",
    "    #\n",
    "    def train(self, num_epochs, loss_fn, optimizer, train_dl, train_size, batch_size, x_valid, y_valid):\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "        # Loss and accuracy history objects initialization\n",
    "        loss_hist_train = [0] * num_epochs\n",
    "        accuracy_hist_train = [0] * num_epochs\n",
    "        loss_hist_valid = [0] * num_epochs\n",
    "        accuracy_hist_valid = [0] * num_epochs\n",
    "        \n",
    "        # Learning loop\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # Batch learn\n",
    "            for x_batch, y_batch in train_dl:\n",
    "                #print('*'*20)\n",
    "                # ---\n",
    "                # 1.4.1. Get the predictions, the [:,0] reshapes from (batch_size,1) to (batch_size)\n",
    "                pred = self(x_batch)[:,0]\n",
    "                # 1.4.2. Compute the loss\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                # 1.4.3. Back propagate the gradients\n",
    "                # The `backward()` method, already available in PyTroch, calculates the \n",
    "                # derivative of the Error in respect to the NN weights\n",
    "                # applying the chain rule for hidden neurons.\n",
    "                loss.backward()\n",
    "                # 1.4.4. Update the weights based on the computed gradients\n",
    "                optimizer.step()\n",
    "                # ---\n",
    "                \n",
    "                # Reset to zero the gradients so they will not accumulate over the mini-batches\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update performance metrics\n",
    "                loss_hist_train[epoch] += loss.item()\n",
    "                is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "                accuracy_hist_train[epoch] += is_correct.mean()\n",
    "                \n",
    "            # Average the results\n",
    "            loss_hist_train[epoch] /= train_size/batch_size\n",
    "            accuracy_hist_train[epoch] /= train_size/batch_size\n",
    "            \n",
    "            # Predict the validation set\n",
    "            pred = self(x_valid)[:, 0]\n",
    "            loss_hist_valid[epoch] = loss_fn(pred, y_valid).item()\n",
    "            is_correct = ((pred>=0.5).float() == y_valid).float()\n",
    "            accuracy_hist_valid[epoch] += is_correct.mean()\n",
    "            \n",
    "        return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid\n",
    "\n",
    "    # Not needed normaly, it is just for mlextend plots\n",
    "    def predict(self, x):\n",
    "        print(f'predict with input shape: {x.shape}')\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        pred = self.forward(x)[:, 0]\n",
    "        print(f'finished predict with output shape: {pred.shape}')\n",
    "        return (pred>=0.5).float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 230.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "exal_NEt_structre_hyperparameters = [3,6,8,5]\n",
    "\n",
    "frist_instance = Net_group_Y(input_size=2, output_size=1, hidden_layer_sizes=exal_NEt_structre_hyperparameters)\n",
    "frist_instance.to(device)\n",
    "loss_fn_instance = nn.BCELoss()\n",
    "learning_rate_parameter = 0.01\n",
    "optimizer_name = 'Adam'\n",
    "\n",
    "optimizer_choiche = {\n",
    "    'GD': torch.optim.SGD(frist_instance.parameters(), lr=learning_rate_parameter),\n",
    "    'SGD': torch.optim.SGD(frist_instance.parameters(), lr=learning_rate_parameter),\n",
    "    'MiniSGD': torch.optim.SGD(frist_instance.parameters(), lr=learning_rate_parameter),\n",
    "    'ASGD': torch.optim.ASGD(frist_instance.parameters(), lr=learning_rate_parameter),\n",
    "    \n",
    "    'RMSprop': torch.optim.RMSprop(frist_instance.parameters(), lr=learning_rate_parameter),\n",
    "    'Adam': torch.optim.Adam(frist_instance.parameters(), lr=learning_rate_parameter)\n",
    "}\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "\n",
    "history = frist_instance.train(\n",
    "    loss_fn=loss_fn_instance, \n",
    "    optimizer=optimizer_choiche[optimizer_name], \n",
    "    num_epochs=num_epochs, \n",
    "    train_dl=train_dl_GD, \n",
    "    train_size=train_size, \n",
    "    batch_size=batch_size_GD,\n",
    "    x_valid=X_val, y_valid=y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 96.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "********************\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n",
      "forward pass layer 1\n",
      "forward pass layer 2\n",
      "forward pass layer 3\n",
      "forward pass layer 4\n",
      "forward pass layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train,X_val,y_train,y_val,\n",
    "                model, num_epochs, loss_fn, optimizer_name, batch_size, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the model with the given parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The neural network model to train.\n",
    "    - num_epochs: Number of epochs to train the model.\n",
    "    - loss_fn: Loss function to use for training.\n",
    "    - optimizer: Optimizer to use for training.\n",
    "    - batch_size: Size of each batch during training.\n",
    "    - x_valid: Validation input data.\n",
    "    - y_valid: Validation target data.\n",
    "    - learning_rate: Learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "    - history: Training history containing loss and accuracy metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define datasets for data loaders\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    train_size = len(train_ds)\n",
    "    if optimizer_name == 'GD':\n",
    "        batch_size = train_size\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "    \n",
    "    elif optimizer_name == 'SGD':\n",
    "        batch_size = 1\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "    else:\n",
    "        batch_size = batch_size\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer_choiche = {\n",
    "    'GD': torch.optim.SGD(model.parameters(), lr=learning_rate),\n",
    "    'SGD': torch.optim.SGD(model.parameters(), lr=learning_rate),\n",
    "    'MiniSGD': torch.optim.SGD(model.parameters(), lr=learning_rate),\n",
    "    'ASGD': torch.optim.ASGD(model.parameters(), lr=learning_rate),\n",
    "    \n",
    "    'RMSprop': torch.optim.RMSprop(model.parameters(), lr=learning_rate),\n",
    "    'Adam': torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    }\n",
    "    optimizer_instance = optimizer_choiche[optimizer_name]\n",
    "    \n",
    "    return model.train(\n",
    "        num_epochs=num_epochs, \n",
    "        loss_fn=loss_fn, \n",
    "        optimizer=optimizer_instance, \n",
    "        train_dl=train_dl, \n",
    "        train_size=train_size, \n",
    "        batch_size=batch_size,\n",
    "        x_valid=train_ds.tensors[0],\n",
    "        y_valid=train_ds.tensors[1])\n",
    "    \n",
    "# Example usage\n",
    "history = train_model(\n",
    "    X_train=X_train, \n",
    "    X_val=X_val, \n",
    "    y_train=y_train, \n",
    "    y_val=y_val,\n",
    "    model=frist_instance,\n",
    "    num_epochs=10,\n",
    "    loss_fn=loss_fn_instance,\n",
    "    optimizer_name=\"Adam\",\n",
    "    batch_size=10,\n",
    "    learning_rate=learning_rate_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6930275122324626,\n",
       "  0.672054918607076,\n",
       "  0.6303154667218526,\n",
       "  0.5575008710225423,\n",
       "  0.45660227139790854,\n",
       "  0.3482097715139389,\n",
       "  0.2670164187749227,\n",
       "  0.2445397838950157,\n",
       "  0.2023410402238369,\n",
       "  0.15560577996075153],\n",
       " [0.6811821460723877,\n",
       "  0.6533634662628174,\n",
       "  0.5925976634025574,\n",
       "  0.499734491109848,\n",
       "  0.3825667202472687,\n",
       "  0.2644290626049042,\n",
       "  0.22350843250751495,\n",
       "  0.2099994719028473,\n",
       "  0.15125267207622528,\n",
       "  0.1303766816854477],\n",
       " [tensor(0.4667),\n",
       "  tensor(0.6400),\n",
       "  tensor(0.6933),\n",
       "  tensor(0.7000),\n",
       "  tensor(0.7667),\n",
       "  tensor(0.8867),\n",
       "  tensor(0.9000),\n",
       "  tensor(0.9333),\n",
       "  tensor(0.9067),\n",
       "  tensor(0.9400)],\n",
       " [tensor(0.5733),\n",
       "  tensor(0.6733),\n",
       "  tensor(0.7000),\n",
       "  tensor(0.7333),\n",
       "  tensor(0.8867),\n",
       "  tensor(0.8933),\n",
       "  tensor(0.9200),\n",
       "  tensor(0.9200),\n",
       "  tensor(0.9333),\n",
       "  tensor(0.9467)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. NN instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the NNs\n",
    "nn_names = ['GD', 'SGD', 'MiniSGD', 'ASGD', 'RMSprop', 'Adam']\n",
    "nn_torch = {}\n",
    "for k in nn_names:\n",
    "    nn_torch.update({\n",
    "        k: Net().to(device)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instantiation method have initialized the NN weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_torch['GD'].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_torch['GD'].fc1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 6. Loss\n",
    "\n",
    "For XOR problem, let's use the standard Binary Crossentropy Loss. \n",
    "Take a look at the PyTorch docs for more available loss functions:\n",
    "<a href='https://pytorch.org/docs/stable/nn.html#loss-functions'>https://pytorch.org/docs/stable/nn.html#loss-functions</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 7. Optimizer\n",
    "\n",
    "In this notebook, let's comapre the performance of the following optimizers:\n",
    "\n",
    "- Gradient Descent (GD)\n",
    "- Stochastic Gadient Descent (SGD)\n",
    "- Mini-batch Gadient Descent (MiniSGD)\n",
    "- Averaged Stochastic Gradient Descent (ASGD)\n",
    "- Root Mean Square Propagation (RMSprop)\n",
    "\n",
    "Notice that the GD, SGD, and MiniSGD use the same optimization algorithm. What will be different is the batch size used for each learning loop iteration. They are available in PyTorch. Take a look at the PyTorch docs for more available optimization algorithms and its hyperparameters:\n",
    "<a href='https://pytorch.org/docs/stable/optim.html#module-torch.optim'>https://pytorch.org/docs/stable/optim.html#module-torch.optim</a>.\n",
    "\n",
    "```python\n",
    "torch.optim.SGD(nn.parameters())\n",
    "torch.optim.ASGD(nn.parameters())\n",
    "torch.optim.RMSprop(nn.parameters())\n",
    "torch.optim.Adam(nn.parameters())\n",
    "```\n",
    "\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = .05\n",
    "\n",
    "# SGD optmizer\n",
    "optimizer = {\n",
    "    'GD': torch.optim.SGD(nn_torch['GD'].parameters(), lr=learning_rate),\n",
    "    'SGD': torch.optim.SGD(nn_torch['SGD'].parameters(), lr=learning_rate),\n",
    "    'MiniSGD': torch.optim.SGD(nn_torch['MiniSGD'].parameters(), lr=learning_rate),\n",
    "    'ASGD': torch.optim.ASGD(nn_torch['ASGD'].parameters(), lr=learning_rate),\n",
    "    \n",
    "    'RMSprop': torch.optim.RMSprop(nn_torch['RMSprop'].parameters(), lr=learning_rate),\n",
    "    'Adam': torch.optim.Adam(nn_torch['Adam'].parameters(), lr=learning_rate)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. NN Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "history_torch = {'GD': nn_torch['GD'].train(\n",
    "    loss_fn=loss_fn, \n",
    "    optimizer=optimizer['GD'], \n",
    "    num_epochs=num_epochs, \n",
    "    train_dl=train_dl_GD, \n",
    "    train_size=train_size, \n",
    "    batch_size=batch_size_GD,\n",
    "    x_valid=X_val, y_valid=y_val\n",
    ")}\n",
    "print('GD train finished\\n')\n",
    "\n",
    "history_torch.update({'SGD': nn_torch['SGD'].train(\n",
    "    loss_fn=loss_fn, \n",
    "    optimizer=optimizer['SGD'], \n",
    "    num_epochs=num_epochs, \n",
    "    train_dl=train_dl_SGD, \n",
    "    train_size=train_size, \n",
    "    batch_size=batch_size_SGD,\n",
    "    x_valid=X_val, y_valid=y_val\n",
    ")})\n",
    "print('SGD train finished\\n')\n",
    "\n",
    "for dl in list(optimizer.keys())[2:]:\n",
    "    history_torch.update({dl: nn_torch[dl].train(\n",
    "        loss_fn=loss_fn, \n",
    "        optimizer=optimizer[dl], \n",
    "        num_epochs=num_epochs, \n",
    "        train_dl=train_dl_MniSGD, \n",
    "        train_size=train_size, \n",
    "        batch_size=batch_size_MiniSGD,\n",
    "        x_valid=X_val, y_valid=y_val\n",
    "    )})\n",
    "    print(dl+' train finished\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9. NN history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_optimizers = len(optimizer)\n",
    "fig = make_subplots(\n",
    "    rows=n_optimizers,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        f'PyTorch {opt_name} - Loss' if i % 2 == 0 else f'Pytorch {opt_name} - Accuracy'\n",
    "        for opt_name in optimizer.keys() for i in range(2)\n",
    "    ],\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "for row_i, (opt_name, _) in enumerate(optimizer.items(), start=1):\n",
    "    hist = history_torch[opt_name]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=hist[0], mode='lines', name='Train Loss',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=hist[1], mode='lines', name='Validation Loss',\n",
    "        line=dict(color='orange'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=[v.cpu().item() for v in hist[2]], mode='lines', name='Train Accuracy',\n",
    "        line=dict(color='forestgreen'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=2)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=[v.cpu().item() for v in hist[3]], mode='lines', name='Validation Accuracy',\n",
    "        line=dict(color='orangered'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=2)\n",
    "\n",
    "    fig.update_yaxes(range=[0, 1.01], row=row_i, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=300 * n_optimizers,\n",
    "    width=800,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=-0.05, xanchor='center', x=0.5),\n",
    "    margin=dict(t=30, b=0)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 10. NN Decision Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "for nn_i, nn_name in enumerate(nn_torch.keys()):\n",
    "    plt.subplot(n_rows, n_cols, nn_i + 1)\n",
    "    plot_decision_regions(X=X_val.cpu().numpy(), \n",
    "                      y=y_val.cpu().int().numpy(),\n",
    "                      clf=nn_torch[nn_name].to('cpu'))\n",
    "    plt.title(nn_name+' PyTorch NN\\nDecision Regions', size=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Play with different NN architectures, learning rates, momentum, number of iterations (epochs) etc*.\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<hr />\n",
    "\n",
    "## 2. Building a Network with TensorFlow\n",
    "\n",
    "Check the <a href='https://www.tensorflow.org/guide'>TensorFlow documentation</a> for details.\n",
    "\n",
    "Neural Network in Tensorflow have a modules structure, and it can be summarised in only two steps:\n",
    "\n",
    "1. Create the NN instance;\n",
    "2. Define the architecture of the network, with the number of neurons, weights initializers (optional), and activation functions of each layer;\n",
    "3. Define the loss function. Take a look at the Tensorflow docs for more available loss functions: <a href='https://www.tensorflow.org/api_docs/python/tf/keras/losses'>https://www.tensorflow.org/api_docs/python/tf/keras/losses</a>;\n",
    "4. Define the optimizer. Take a look at the Tensorflow docs for more available optimizers: <a href='https://www.tensorflow.org/api_docs/python/tf/keras/optimizers'>https://www.tensorflow.org/api_docs/python/tf/keras/optimizers</a>;\n",
    "5. Compile the model. At this moment, the loss and the optimizer are set, together with other settings, like extra metrics to be evaluated at each epoch.\n",
    "\n",
    "In step 1, a new model is instanciated with the Class of the type of NN that is needed. To this object, new modules are added to define the NN architecture.\n",
    "\n",
    "In setp 2, the weights and bias are, by deafult, initialized with the Xavier uniform (called Glorot Uniform in Tensorflow) and the zeros method, repectively. However, the initializers can be chosen from the available methods in Tensorflow framework. Take a look at the Tensorflow docs for more available initialization methods:\n",
    "<a href='https://www.tensorflow.org/api_docs/python/tf/keras/initializers/'>https://www.tensorflow.org/api_docs/python/tf/keras/initializers/</a>.\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')\n",
    "print('Num GPUs Available: ', len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. NN definitions and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Steps 1 and 2\n",
    "#\n",
    "# NN architecture\n",
    "with tf.device('/cpu:0'):\n",
    "    m = Sequential()\n",
    "    m.add(keras.Input(shape=(X_train.shape[1], )))\n",
    "    # In this layer, the weights and bias initializer are explicitly set\n",
    "    m.add(Dense(3, \n",
    "                activation='relu',\n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                bias_initializer=tf.keras.initializers.Zeros())),\n",
    "    # In this layer, just as an example, the weights and bias initializer are NOT explicitly set\n",
    "    m.add(Dense(4, activation='relu')),\n",
    "    # In this layer, the weights and bias initializer are not set.\n",
    "    m.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #\n",
    "    # Steps 3 to 5\n",
    "    #\n",
    "    # Compile models NNs\n",
    "    nn_tf = {}\n",
    "    for k in ['GD', 'SGD', 'MiniSGD', 'RMSprop', 'Adam']:\n",
    "        nn_tf.update({k: tf.keras.models.clone_model(m)})\n",
    "    \n",
    "    nn_tf['GD'].compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    nn_tf['SGD'].compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    nn_tf['MiniSGD'].compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    nn_tf['RMSprop'].compile(optimizer=RMSprop(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    nn_tf['Adam'].compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. NN fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking batch sizes\n",
    "[batch_size_GD, batch_size_SGD, batch_size_MiniSGD]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_tf = {}\n",
    "\n",
    "print('Training GD model... ', end='')\n",
    "history_tf['GD'] = nn_tf['GD'].fit(\n",
    "    x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size_GD, verbose=0\n",
    ")\n",
    "print('Finished.')\n",
    "\n",
    "print('Training SGD model... ', end='')\n",
    "history_tf['SGD'] = nn_tf['SGD'].fit(\n",
    "    x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size_SGD, verbose=0\n",
    ")\n",
    "print('Finished.')\n",
    "\n",
    "print('Training MiniSGD model... ', end='')\n",
    "history_tf['MiniSGD'] = nn_tf['MiniSGD'].fit(\n",
    "    x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size_MiniSGD, verbose=0\n",
    ")\n",
    "print('Finished.')\n",
    "\n",
    "print('Training RMSprop model Training... ', end='')\n",
    "history_tf['RMSprop'] = nn_tf['RMSprop'].fit(\n",
    "    x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size_MiniSGD, verbose=0\n",
    ")\n",
    "print('Finished.')\n",
    "\n",
    "print('Training Adam model Training... ', end='')\n",
    "history_tf['Adam'] = nn_tf['Adam'].fit(\n",
    "    x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size_MiniSGD, verbose=0\n",
    ")\n",
    "print('Finished.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. NN history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_optimizers = len(history_tf)\n",
    "fig = make_subplots(\n",
    "    rows=n_optimizers,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        f'Tensorflow {opt_name} - Loss' if i % 2 == 0 else f'Tensorflow {opt_name} - Accuracy'\n",
    "        for opt_name in history_tf.keys() for i in range(2)\n",
    "    ],\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "for row_i, (opt_name, hist) in enumerate(history_tf.items(), start=1):\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=hist.history['loss'], mode='lines', name='Train Loss',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=hist.history['val_loss'], mode='lines', name='Validation Loss',\n",
    "        line=dict(color='orange'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=hist.history['accuracy'], mode='lines', name='Train Accuracy',\n",
    "        line=dict(color='forestgreen'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=2)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=hist.history['val_accuracy'], mode='lines', name='Validation Accuracy',\n",
    "        line=dict(color='orangered'),\n",
    "        showlegend=(row_i == 1)\n",
    "    ), row=row_i, col=2)\n",
    "\n",
    "    fig.update_yaxes(range=[0, 1.01], row=row_i, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=300 * n_optimizers,\n",
    "    width=800,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=-0.05, xanchor='center', x=0.5),\n",
    "    margin=dict(t=50, b=50)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 4. NN Decision Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "for nn_i, nn_name in enumerate(nn_tf.keys()):\n",
    "    plt.subplot(n_rows, n_cols, nn_i+1)\n",
    "    plot_decision_regions(X=X_val.cpu().numpy(), \n",
    "                      y=y_val.cpu().int().numpy(),\n",
    "                      clf=nn_tf[nn_name])\n",
    "    plt.title(nn_name+' Tensorflow NN\\nDecision Regions', size=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Play with different NN architectures, learning rates, momentum, number of iterations (epochs) etc*.\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## 3. Excercises (not graded)\n",
    "\n",
    "- Try different hyperparameters, such as network architecture and learning rates, to improve the NN performance.\n",
    "- Try different random states to analyse the randomness of the results.\n",
    "- Fit the NN for a regression dataset.\n",
    "    \n",
    "<br />\n",
    "\n",
    "\n",
    "## 4. Check it out\n",
    "\n",
    "This website has a very nice tool for testing the effect of the NN hyperparameters on NN results:\n",
    "\n",
    "<br />\n",
    "\n",
    "<center>\n",
    "<h4><a href='https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-gauss&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,1,2&seed=0.47596&showTestData=false&discretize=false&percTrainData=30&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false', target='_blank'>Neural Network Playground</a></h4>\n",
    "</center>\n",
    "\n",
    "<br />\n",
    "Enjoy!\n",
    "\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CIFO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
