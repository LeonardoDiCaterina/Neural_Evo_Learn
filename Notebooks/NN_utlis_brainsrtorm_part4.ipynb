{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Networks - Learning and Implementation strategies\n",
    "\n",
    "In this notebook, we will study the use of `PyTorch` and `Tensorflow` frameworks for implementing and training Neural Networks. This is not intended to be exhaustive, but rather to provide examples for exploring the algorithms and their hyperparameters with these frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from mlxtend.plotting import plot_decision_regions'''\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "\n",
    "criterion = RMSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Arc(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=2, output_size=1,\n",
    "                 hidden_layer_sizes=[3,4,5], activation=nn.ReLU()):\n",
    "        \n",
    "        super(Net_Arc, self).__init__()\n",
    "        #\n",
    "        # 1. 1. Network architecture\n",
    "        \n",
    "        self.add_module(f'fc{1}', nn.Linear(input_size, hidden_layer_sizes[0]))\n",
    "        \n",
    "        for i in range(1,len(hidden_layer_sizes)):\n",
    "            self.add_module(f'fc{i+1}', nn.Linear(hidden_layer_sizes[i-1], hidden_layer_sizes[i]))\n",
    "        \n",
    "        self.add_module(f'fc{len(hidden_layer_sizes) +1 }', nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "\n",
    "        # Weights initialisation\n",
    "        # The apply method applies the function passed as the apply() argument\n",
    "        # to each element in the object, that in this case is the neural network.\n",
    "        self.apply(self._init_weights)\n",
    "        # Store the parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.n_forward_calls = 0\n",
    "        \n",
    "    #\n",
    "    # 1. 2. Weights and bias initialization\n",
    "    #\n",
    "    def _init_weights(self, attribute):\n",
    "        if isinstance(attribute, nn.Linear):\n",
    "          torch.nn.init.xavier_uniform_(attribute.weight)\n",
    "          torch.nn.init.zeros_(attribute.bias)\n",
    "    #\n",
    "    # 1. 3. Forward pass\n",
    "    \"\"\"    \n",
    "    def forward(self, x):\n",
    "        # For each layer, the output will be the ReLu activation applied to the output of the linear operation\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # For the last layer, the sigmoid function will be the activation\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through all layers\n",
    "        for i in range(1, len(self.hidden_layer_sizes) + 2):\n",
    "            #print(f'forward pass layer {i}')\n",
    "            layer = getattr(self, f'fc{i}')\n",
    "            x = layer(x)\n",
    "            if i < len(self.hidden_layer_sizes):\n",
    "                self.n_forward_calls += 1\n",
    "                x = self.activation(x)\n",
    "        # Apply sigmoid activation to the output layer\n",
    "        self.n_forward_calls += 1\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    #\n",
    "    # 1. 4. Training loop\n",
    "    # For details, see Machine Learning with PyTorch and Scikit-Learn.\n",
    "    #\n",
    "    def train(self, num_epochs, loss_fn, optimizer, train_dl, train_size, batch_size, x_valid, y_valid):\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        n_calls_array = np.zeros(num_epochs)\n",
    "    \n",
    "        # Loss and accuracy history objects initialization\n",
    "        loss_hist_train = [0] * num_epochs\n",
    "        accuracy_hist_train = [0] * num_epochs\n",
    "        loss_hist_valid = [0] * num_epochs\n",
    "        accuracy_hist_valid = [0] * num_epochs\n",
    "        delta_times = [0] * num_epochs\n",
    "        self.n_forward_calls = 0\n",
    "        \n",
    "        # Learning loop\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            start_time = time.time()\n",
    "            # Batch learn\n",
    "            for x_batch, y_batch in train_dl:\n",
    "                #print('*'*20)\n",
    "                # ---\n",
    "                # 1.4.1. Get the predictions, the [:,0] reshapes from (batch_size,1) to (batch_size)\n",
    "                pred = self(x_batch)[:,0]\n",
    "                # 1.4.2. Compute the loss\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                # 1.4.3. Back propagate the gradients\n",
    "                # The `backward()` method, already available in PyTroch, calculates the \n",
    "                # derivative of the Error in respect to the NN weights\n",
    "                # applying the chain rule for hidden neurons.\n",
    "                loss.backward()\n",
    "                # 1.4.4. Update the weights based on the computed gradients\n",
    "                optimizer.step()\n",
    "                # ---\n",
    "                \n",
    "                # Reset to zero the gradients so they will not accumulate over the mini-batches\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update performance metrics\n",
    "                loss_hist_train[epoch] += loss.item()\n",
    "                is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "                accuracy_hist_train[epoch] += is_correct.mean()\n",
    "                \n",
    "            n_calls_array[epoch] = self.n_forward_calls\n",
    "            self.n_forward_calls = 0\n",
    "            delta_times[epoch] = time.time() - start_time\n",
    "            # Average the results\n",
    "            loss_hist_train[epoch] /= train_size/batch_size\n",
    "            accuracy_hist_train[epoch] /= train_size/batch_size\n",
    "            \n",
    "            # Predict the validation set\n",
    "            pred = self(x_valid)[:, 0]\n",
    "            loss_hist_valid[epoch] = loss_fn(pred, y_valid).item()\n",
    "            is_correct = ((pred>=0.5).float() == y_valid).float()\n",
    "            accuracy_hist_valid[epoch] += is_correct.mean()\n",
    "            \n",
    "        return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid, n_calls_array, delta_times\n",
    "\n",
    "    # Not needed normaly, it is just for mlextend plots\n",
    "    def predict(self, x):\n",
    "        print(f'predict with input shape: {x.shape}')\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        pred = self.forward(x)[:, 0]\n",
    "        print(f'finished predict with output shape: {pred.shape}')\n",
    "        return (pred>=0.5).float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_model():\n",
    "    def __init__(self,\n",
    "                X_train,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_val,\n",
    "                input_size, \n",
    "                output_size,\n",
    "                hidden_layer_sizes, \n",
    "                optimizer_name:str,\n",
    "                num_epochs,\n",
    "                train_dl: DataLoader,  \n",
    "                train_size,\n",
    "                batch_size,\n",
    "                learning_rate,\n",
    "                log_path:str,\n",
    "                log_level,\n",
    "                seed = 42,\n",
    "                activation = nn.ReLU(),\n",
    "                loss_fn = nn.BCELoss()\n",
    "                ):\n",
    "        \n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.num_epochs = num_epochs\n",
    "        self.train_dl = train_dl \n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.log_path = log_path\n",
    "        self.seed = seed \n",
    "        self.activation = activation\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        \n",
    "        self.arcthicture = Net_Arc(input_size=input_size,\n",
    "                                  output_size=output_size,\n",
    "                                  hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                  activation=activation)\n",
    "        \n",
    "        self.arcthicture.to(device)\n",
    "        \n",
    "        \n",
    "        self.fitnesses, self.test_fitnesses, self.accuracy, self.accuracy_valid, self.array_forward_calls, self.delta_times = self.fit(\n",
    "                X_train,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_val,\n",
    "                num_epochs=num_epochs, \n",
    "                loss_fn=loss_fn, \n",
    "                optimizer_name=optimizer_name,\n",
    "                batch_size=batch_size, \n",
    "                learning_rate=learning_rate,\n",
    "                seed=seed)\n",
    "    \n",
    "        if log_level == 2:\n",
    "            self.logger(self.log_path)\n",
    "\n",
    "\n",
    "        self.fitness = torch.tensor(self.fitnesses[-1])\n",
    "        self.test_fitness = torch.tensor(self.test_fitnesses[-1])\n",
    "\n",
    "\n",
    "    def logger(self, log_path): \n",
    "        \n",
    "\n",
    "        df = pd.DataFrame(index=range(self.num_epochs))\n",
    "        df['algorithm'] = str(self.optimizer_name)\n",
    "        df['Instance ID'] = 1 #PLACEHOLDER\n",
    "        df['dataset'] = 2 #PLACEHOLDER \n",
    "        df['seed'] = self.seed\n",
    "        df['epochs'] = range(1, self.num_epochs + 1)\n",
    "        df['fitness'] = self.fitnesses\n",
    "        df['running time'] = self.delta_times\n",
    "        df['population nodes'] = 7 #PLACEHOLDER\n",
    "        df['test_fitness'] = self.test_fitnesses\n",
    "        df['Elite nodes'] = 9 #PLACEHOLDER\n",
    "        df['niche entropy'] = 10 #PLACEHOLDER\n",
    "        df['sd(pop.fit)'] = 11 #PLACEHOLDER\n",
    "        df['Log Level'] = 12 #PLACEHOLDER\n",
    "        df['params'] = 13 #PLACEHOLDER\n",
    "        df['n_forward_calls'] = self.array_forward_calls.tolist()\n",
    "\n",
    "\n",
    "        # If \n",
    "        df.to_csv(log_path, index=False, header = False)\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "    def fit(self,X_train,X_val,y_train,y_val,\n",
    "                num_epochs, loss_fn, optimizer_name, batch_size, learning_rate, seed):\n",
    "        \"\"\"\n",
    "        Train the model with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_epochs: Number of epochs to train the model.\n",
    "        - loss_fn: Loss function to use for training.\n",
    "        - optimizer: Optimizer to use for training.\n",
    "        - batch_size: Size of each batch during training.\n",
    "        - x_valid: Validation input data.\n",
    "        - y_valid: Validation target data.\n",
    "        - learning_rate: Learning rate for the optimizer.\n",
    "        \n",
    "        Returns:\n",
    "        - history: Training history containing loss and accuracy metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Define datasets for data loaders\n",
    "        train_ds = TensorDataset(X_train, y_train)\n",
    "        val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "        train_size = len(train_ds)\n",
    "        if optimizer_name == 'GD':\n",
    "            batch_size = train_size\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "            #val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "        \n",
    "        elif optimizer_name == 'SGD':\n",
    "            batch_size = 1\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "            #val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "\n",
    "        else:\n",
    "            batch_size = batch_size\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "            #val_dl = DataLoader(val_ds, batch_size, shuffle=True)\n",
    "        \n",
    "        optimizer_choiche = {\n",
    "        'GD': torch.optim.SGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'SGD': torch.optim.SGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'MiniSGD': torch.optim.SGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'ASGD': torch.optim.ASGD(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'RMSprop': torch.optim.RMSprop(self.arcthicture.parameters(), lr=learning_rate),\n",
    "        'Adam': torch.optim.Adam(self.arcthicture.parameters(), lr=learning_rate)\n",
    "        }\n",
    "        optimizer_instance = optimizer_choiche[optimizer_name]\n",
    "        \n",
    "        return self.arcthicture.train(\n",
    "            num_epochs=num_epochs, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optimizer_instance, \n",
    "            train_dl=train_dl, \n",
    "            train_size=train_size, \n",
    "            batch_size=batch_size,\n",
    "            x_valid=X_val,     \n",
    "            y_valid=y_val )\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes\n",
      "[torch.Size([150, 2]), torch.Size([150])] [torch.float32, torch.float32] [device(type='cpu'), device(type='cpu')]\n",
      "[torch.Size([50, 2]), torch.Size([50])] [torch.float32, torch.float32] [device(type='cpu'), device(type='cpu')]\n",
      "\n",
      "Batch sizes\n",
      "150\n",
      "1\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Data simulation\n",
    "N_data = 200\n",
    "train_size = 150\n",
    "X = 2 * torch.rand(N_data, 2, device=device, dtype=torch.float32) - 1\n",
    "y = torch.tensor([0 if elem[0]*elem[1] < 0 else 1 for elem in X], device=device, dtype=torch.float32)\n",
    "\n",
    "# Split training and test partitions\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:, :]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "# Define datasets for data loaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size_GD = train_size\n",
    "train_dl_GD = DataLoader(train_ds, batch_size_GD, shuffle=True)\n",
    "val_dl_GD = DataLoader(val_ds, batch_size_GD, shuffle=True)\n",
    "\n",
    "batch_size_SGD = 1\n",
    "train_dl_SGD = DataLoader(train_ds, batch_size_SGD, shuffle=True)\n",
    "val_dl_SGD = DataLoader(val_ds, batch_size_SGD, shuffle=True)\n",
    "\n",
    "batch_size_MiniSGD = 32\n",
    "train_dl_MniSGD = DataLoader(train_ds, batch_size_MiniSGD, shuffle=True)\n",
    "val_dl_MiniSGD = DataLoader(val_ds, batch_size_MiniSGD, shuffle=True)\n",
    "\n",
    "print('Partition sizes')\n",
    "print([X_train.shape, y_train.shape], [X_train.dtype, y_train.dtype], [X_train.device, y_train.device])\n",
    "print([X_val.shape, y_val.shape], [X_val.dtype, y_val.dtype], [X_val.device, y_val.device])\n",
    "\n",
    "print('\\nBatch sizes')\n",
    "print(batch_size_GD)\n",
    "print(batch_size_SGD)\n",
    "print(batch_size_MiniSGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os \n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "# Edit the name and log directory based on the model you want to run\n",
    "\n",
    "#MODEL_NAME = 'GP'\n",
    "#MODEL_NAME = 'GSGP'\n",
    "#MODEL_NAME = 'SLIM-GSGP'\n",
    "MODEL_NAME = 'NN'\n",
    "\n",
    "DATASET_NAME = MODEL_NAME +'_sustavianfeed'\n",
    "LOG_DIR = './log/' + MODEL_NAME + '/'\n",
    "\n",
    "LOG_LEVEL = 2\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 177.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model = nn_model(X_train = X_train,\n",
    "         X_val = X_val,\n",
    "         y_train = y_train,\n",
    "        y_val = y_val,\n",
    "        input_size = 2, \n",
    "        output_size = 1,\n",
    "        hidden_layer_sizes = [2,3,4], \n",
    "        optimizer_name= 'GD',\n",
    "        num_epochs = 15,\n",
    "        train_dl = None,  \n",
    "        train_size = train_size,\n",
    "        batch_size = batch_size_GD,\n",
    "        learning_rate = 0.01,\n",
    "        seed = seed,\n",
    "        log_path = LOG_DIR + DATASET_NAME + \".csv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the time of the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irism\\OneDrive - NOVAIMS\\Msc-NEL\\Neural_Evo_Learn\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(os.path.join(os.getcwd(), os.pardir))\n",
    "%cd notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_times = {}\n",
    "for model in ['SLIM-GSGP', 'GSGP', 'GP']:\n",
    "    total_time = 0\n",
    "    for i_outer in range(10):\n",
    "        df = pd.read_csv(f\"./log/{model}/{model}_sustavianfeed_outer_{i_outer}.csv\")\n",
    "        total_time += df.iloc[:, 6].sum()\n",
    "    model_times[model] = total_time\n",
    "\n",
    "model_times_df = pd.DataFrame.from_dict(model_times, orient='index', columns=['Total Time (s)'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM-GSGP</th>\n",
       "      <td>24.840032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSGP</th>\n",
       "      <td>44.637842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GP</th>\n",
       "      <td>144.934460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Total Time (s)\n",
       "SLIM-GSGP       24.840032\n",
       "GSGP            44.637842\n",
       "GP             144.934460"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_times_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
